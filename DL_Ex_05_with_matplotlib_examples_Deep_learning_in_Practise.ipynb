{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL-Ex-05-with-matplotlib examples-Deep-learning-in-Practise.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCu8i38pRWI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8_vftIIr5se",
        "colab_type": "text"
      },
      "source": [
        "# 1.\tLogistic regression ( Binary or multi-class)\n",
        "\n",
        "Let's consider Binary classification \n",
        "\n",
        "\t- Classify movie reviews as positive or negative, based on the text content of the reviews (binary cls)\n",
        "\t\t- data: Use standard IMDB available from keras\n",
        "\t\t- Prepare data and build model\n",
        "\t\t- Train and Validate\n",
        "\t\t- Validate your approach by plotting validation and training results \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyOUzLR4r3uX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAzgEr1gr3rF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsjiRc9aKrvV",
        "colab_type": "text"
      },
      "source": [
        "# IMDB Dataset\n",
        "\n",
        "A set of 50,000 highly polarized reviews from the\n",
        "Internet Movie Database. \n",
        "\n",
        "They’re split into 25,000 reviews for training and 25,000\n",
        "reviews for testing. \n",
        "\n",
        "Each set consisting of 50% negative and 50% positive reviews.\n",
        "\n",
        "\n",
        "\n",
        " **The IMDB dataset comes packaged with Keras.**\n",
        "\n",
        "It has already been preprocessed: the reviews (sequences of words) have been turned into\n",
        "sequences of integers, where each integer stands for a specific word in a dictionary.\n",
        "\n",
        "\n",
        "**Developers of keras says: 'For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWQozPA7gUiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5LyiFAKr3ni",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "outputId": "fd701470-8a39-49ea-c98d-4d44b859ee01"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
        "num_words=10000)        # The argument num_words=10000 means you’ll only keep the top 10,000 most \n",
        "                      #       fre-quently occurring words in the training data.\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iircyLfTMaGF",
        "colab_type": "text"
      },
      "source": [
        "The variables train_data and test_data are lists of reviews; \n",
        "\n",
        "each review is a list of word indices (encoding a sequence of words). \n",
        "\n",
        "train_labels and test_labels are\n",
        "lists of 0s and 1s, where 0 stands for negative and 1 stands for positive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxZQdC1iMF1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_data[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElUiAKqVMF8o",
        "colab_type": "code",
        "outputId": "43241b4e-80e0-4fac-a307-684d94252def",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(train_data))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q10rtr9MMGBF",
        "colab_type": "code",
        "outputId": "6851dbd4-e0e5-40c8-dfd8-209ee1a373d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(train_data[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L790UlHFMGE8",
        "colab_type": "code",
        "outputId": "37b1b965-b4b6-4e24-81f3-ecadd52463a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(train_data[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0Da-otyNzWe",
        "colab_type": "code",
        "outputId": "21a4c576-32ab-4582-a11a-2a61b41e4272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_labels[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE-_k3xacw0u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "438060a8-a47b-43ef-8ee5-bde86163bb00"
      },
      "source": [
        "print(len(test_data))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsSykTuceuMC",
        "colab_type": "text"
      },
      "source": [
        "# We will explore data !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwIp611De1OY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "data = np.concatenate((train_data, test_data), axis=0)\n",
        "targets = np.concatenate((train_labels, test_labels), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1UvBP7qfF7x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d5abeca5-f1a3-4916-c320-2e141875a386"
      },
      "source": [
        "print(\"Categories:\", np.unique(targets))\n",
        "print(\"Number of unique words:\", len(np.unique(np.hstack(data))))\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Categories: [0 1]\n",
            "Number of unique words: 9998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfW_88JSfGCQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "46a70f63-8cde-4e5a-f0d7-1bf27f22ce45"
      },
      "source": [
        "length = [len(i) for i in data]\n",
        "print(\"Average Review length:\", np.mean(length))\n",
        "print(\"Standard Deviation:\", round(np.std(length)))\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Review length: 234.75892\n",
            "Standard Deviation: 173.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "052r-6KnfGAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54YUTk8NfF6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q25YaF3thbCg",
        "colab_type": "text"
      },
      "source": [
        "# Let's look at a single training example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKFmaH04fF2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t3BSrHOhdtT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ac04d4e1-a03d-45f2-ac35-b73b8e3e864a"
      },
      "source": [
        "print(\"Label:\", targets[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T3cKe4whdrR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "98273540-5fd0-4478-b17f-c23f7253d5a5"
      },
      "source": [
        "print(data[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDRSqYwVjdzR",
        "colab_type": "text"
      },
      "source": [
        "**The code below retrieves the dictionary mapping word indices back into the original words so that we can read them.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU9azESXfFzc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "e761cd03-c3b0-4c79-e567-f308447f5f35"
      },
      "source": [
        "index = imdb.get_word_index()\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
        "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in data[0]] )\n",
        "print(decoded) "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "# this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for # and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also # to the two little boy's that played the # of norman and paul they were just brilliant children are often left out of the # list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0GBPVemjRsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsajHFxejRnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXsbgglajRiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYMFu-C2N2et",
        "colab_type": "text"
      },
      "source": [
        "# Preparing the data\n",
        "\n",
        "We can’t feed lists of integers into a neural network. \n",
        "\n",
        "We have to turn your lists into\n",
        "tensors. \n",
        "\n",
        "      1. One-hot encode your lists to turn them into vectors of\n",
        "       0s and 1s. \n",
        "\n",
        "      This would mean, for instance, turning the sequence [3, 5] \n",
        "      into a 10,000-dimensional vector that would be all 0s \n",
        "      except for indices 3 and 5, which would be 1s. \n",
        "      \n",
        "      Then you could use as the first layer in your network a\n",
        "       Dense layer, capable of handling floating-point vector \n",
        "       data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YsbmevwNzcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing the data\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension))    # Creates an all-zero matrix of shape (len(sequences),dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1.                         # Sets specific indices of results[i] to 1s\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_MGF9wuNzbR",
        "colab_type": "code",
        "outputId": "efac8609-a832-4ca4-9963-86dea0705d93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Here’s what the samples look like now:\n",
        "x_train[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 1., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzJ6h4ZrNzTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  We should also vectorize our labels, which is straightforward:\n",
        "\n",
        "\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkGrMrR3NzOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFHielvjSjQQ",
        "colab_type": "text"
      },
      "source": [
        "# Building network \n",
        "\n",
        "      - The input data is vectors, and the labels are scalars \n",
        "      (1s and 0s):\n",
        "\n",
        "      - We will use a simple stack of fully connected ( Dense )\n",
        "       layers with relu activations: Dense(16,activation='relu').\n",
        "\n",
        "      - each Dense layer (16) is the number of hidden units of \n",
        "      the layer.\n",
        "\n",
        "      - Essential operation will be : \n",
        "          output = relu(dot(W, input) + b)\n",
        "\n",
        "      - Think what would be the dimension of weight matrix here ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g94JbiVEUxSB",
        "colab_type": "text"
      },
      "source": [
        "So, our architecture will have: \n",
        "\n",
        "     \n",
        "      - Two intermediate layers with 16 hidden units each\n",
        "      - A third layer that will output the scalar prediction \n",
        "      regarding the sentiment of the current review\n",
        "\n",
        "\n",
        "\n",
        "The intermediate layers will use relu as their activation function \n",
        "\n",
        "The final layer will use a sigmoid activation so as to output a probability (a score between 0 and 1, indicating how likely the sample is to have the target “1”: how likely the review is to be\n",
        "positive)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNP2sqhTShH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEmpEm_fWFag",
        "colab_type": "text"
      },
      "source": [
        "# Additional information \n",
        "\n",
        "### Let's see what **françois chollet** says: \n",
        "\n",
        "##### What are activation functions, and why are they necessary?\n",
        "\n",
        "\n",
        "Without an activation function like relu (also called a non-linearity), the Dense layer\n",
        "would consist of two linear operations—a dot product and an addition:\n",
        "\n",
        "output = dot(W, input) + b\n",
        "\n",
        "\n",
        "So the layer could only learn linear transformations (affine transformations) of the\n",
        "input data: the hypothesis space of the layer would be the set of all possible linear\n",
        "transformations of the input data into a 16-dimensional space. \n",
        "\n",
        "Such a hypothesis\n",
        "space is too restricted and wouldn’t benefit from multiple layers of representations,\n",
        "because a deep stack of linear layers would still implement a linear operation: adding\n",
        "more layers wouldn’t extend the hypothesis space.\n",
        "\n",
        "\n",
        "In order to get access to a much richer hypothesis space that would benefit from\n",
        "deep representations, you need a non-linearity, or activation function. \n",
        "\n",
        "relu is the\n",
        "most popular activation function in deep learning, but there are many other candi-\n",
        "dates, which all come with similarly strange names: prelu, elu, and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoGQW9E5ShLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6eQDDF4ShEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWU9fWFRShCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As9it8U1Sg-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGJo2EWEMGQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anLgbLlbrrdg",
        "colab_type": "code",
        "outputId": "dff6b29b-c022-4df6-f034-ea6443f6984b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.datasets import imdb\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
        "num_words=10000)      # The argument num_words=10000 means you’ll only keep the top 10,000 most \n",
        "                      #       fre-quently occurring words in the training data.\n",
        "\n",
        "\n",
        "\n",
        "# Preparing the data\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension))    # Creates an all-zero matrix of shape (len(sequences),dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1.                         # Sets specific indices of results[i] to 1s\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "#  We should also vectorize your labels, which is straightforward:\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')\n",
        "\n",
        "\n",
        "\n",
        "# Model defintion \n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "#Validating your approach\n",
        "#In order to monitor during training the accuracy of the model on data it has never\n",
        "#seen before, you’ll create a validation set by setting apart 10,000 samples from the\n",
        "#original training data.\n",
        "\n",
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "# Crossentropy is a quantity from the field of Information Theory \n",
        "                                        # that measures the distance between probability distributions or, in this\n",
        "                                        #case, between the ground-truth distribution and your predictions.\n",
        "\n",
        "\n",
        "\n",
        "# You can also configure the optimizers as below !!!!\n",
        "\n",
        "#from keras import optimizers\n",
        "#model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
        "#loss='binary_crossentropy',\n",
        "#metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Using custom losses and metrics\n",
        "\n",
        "#from keras import losses\n",
        "#from keras import metrics\n",
        "#model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
        "#loss=losses.binary_crossentropy,\n",
        "#metrics=[metrics.binary_accuracy])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit(partial_x_train,partial_y_train,epochs=20,batch_size=512,validation_data=(x_val, y_val))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 2s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "15000/15000 [==============================] - 6s 375us/step - loss: 0.5243 - acc: 0.7779 - val_loss: 0.3875 - val_acc: 0.8605\n",
            "Epoch 2/20\n",
            "15000/15000 [==============================] - 1s 87us/step - loss: 0.3019 - acc: 0.9023 - val_loss: 0.3135 - val_acc: 0.8779\n",
            "Epoch 3/20\n",
            "15000/15000 [==============================] - 1s 87us/step - loss: 0.2238 - acc: 0.9269 - val_loss: 0.2790 - val_acc: 0.8912\n",
            "Epoch 4/20\n",
            "15000/15000 [==============================] - 1s 87us/step - loss: 0.1734 - acc: 0.9453 - val_loss: 0.2769 - val_acc: 0.8883\n",
            "Epoch 5/20\n",
            "15000/15000 [==============================] - 1s 88us/step - loss: 0.1451 - acc: 0.9533 - val_loss: 0.2834 - val_acc: 0.8863\n",
            "Epoch 6/20\n",
            "15000/15000 [==============================] - 1s 86us/step - loss: 0.1188 - acc: 0.9636 - val_loss: 0.2966 - val_acc: 0.8844\n",
            "Epoch 7/20\n",
            "15000/15000 [==============================] - 1s 87us/step - loss: 0.1003 - acc: 0.9690 - val_loss: 0.3491 - val_acc: 0.8769\n",
            "Epoch 8/20\n",
            "15000/15000 [==============================] - 1s 85us/step - loss: 0.0795 - acc: 0.9783 - val_loss: 0.3403 - val_acc: 0.8793\n",
            "Epoch 9/20\n",
            "15000/15000 [==============================] - 1s 90us/step - loss: 0.0679 - acc: 0.9815 - val_loss: 0.4001 - val_acc: 0.8657\n",
            "Epoch 10/20\n",
            "15000/15000 [==============================] - 1s 87us/step - loss: 0.0572 - acc: 0.9854 - val_loss: 0.3834 - val_acc: 0.8761\n",
            "Epoch 11/20\n",
            "15000/15000 [==============================] - 1s 87us/step - loss: 0.0454 - acc: 0.9897 - val_loss: 0.4471 - val_acc: 0.8713\n",
            "Epoch 12/20\n",
            "15000/15000 [==============================] - 1s 87us/step - loss: 0.0393 - acc: 0.9911 - val_loss: 0.4479 - val_acc: 0.8741\n",
            "Epoch 13/20\n",
            "15000/15000 [==============================] - 1s 87us/step - loss: 0.0332 - acc: 0.9930 - val_loss: 0.4598 - val_acc: 0.8720\n",
            "Epoch 14/20\n",
            "15000/15000 [==============================] - 1s 87us/step - loss: 0.0242 - acc: 0.9959 - val_loss: 0.4936 - val_acc: 0.8718\n",
            "Epoch 15/20\n",
            "15000/15000 [==============================] - 1s 87us/step - loss: 0.0241 - acc: 0.9947 - val_loss: 0.5141 - val_acc: 0.8703\n",
            "Epoch 16/20\n",
            "15000/15000 [==============================] - 1s 87us/step - loss: 0.0200 - acc: 0.9956 - val_loss: 0.5434 - val_acc: 0.8691\n",
            "Epoch 17/20\n",
            "15000/15000 [==============================] - 1s 89us/step - loss: 0.0102 - acc: 0.9991 - val_loss: 0.5893 - val_acc: 0.8587\n",
            "Epoch 18/20\n",
            "15000/15000 [==============================] - 1s 87us/step - loss: 0.0151 - acc: 0.9965 - val_loss: 0.5963 - val_acc: 0.8665\n",
            "Epoch 19/20\n",
            "15000/15000 [==============================] - 1s 86us/step - loss: 0.0064 - acc: 0.9997 - val_loss: 0.6248 - val_acc: 0.8648\n",
            "Epoch 20/20\n",
            "15000/15000 [==============================] - 1s 86us/step - loss: 0.0100 - acc: 0.9980 - val_loss: 0.6549 - val_acc: 0.8659\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXDGYIjuUHu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rgjm9E6ZZQzA",
        "colab_type": "text"
      },
      "source": [
        "Note that the call to model.fit() returns a **History object**. This object has a member history , which is a dictionary containing data about everything that happened\n",
        "during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6ihk8YwU_Vs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtIqJJp2WW4T",
        "colab_type": "code",
        "outputId": "e6ebc124-a18f-4f9f-bfdf-539725572295",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY8UpJo-Wssc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS4r877AZ8Ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8MBhmv2Z8SB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzb0QoqfZ9kp",
        "colab_type": "text"
      },
      "source": [
        "Let's use **Matplotlib** to plot\n",
        "the training and validation loss side by side , as well as the training and\n",
        "validation accuracy . \n",
        "\n",
        "\n",
        "**Note that your own results may vary slightly due to a different random initialization of your network.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv0ZIiGTZ8Oe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaVrlBWEU_kd",
        "colab_type": "code",
        "outputId": "9775b67e-7752-48b7-d02e-42c1004744b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "history_dict = history.history\n",
        "\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOCUCEZEcp0QgIpLj/j+ymsTvgFBmiAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu2igbwqVRcK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmlOKTF4X8bS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QIQROS-X80G",
        "colab_type": "code",
        "outputId": "67b4a380-1c00-4489-e8b8-0b53cf1273ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.clf()   # clear figure\n",
        "acc_values = history_dict['acc']\n",
        "val_acc_values = history_dict['val_acc']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUoIQEZFYShAiIhLr/wN00rdIW4WulgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPI2G-c2X8XA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUdOWy0pYG2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv49flS8byZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO6idqcZby35",
        "colab_type": "text"
      },
      "source": [
        "# Looking into the nature of graphs: \n",
        "\n",
        "This is an example of **overfit case**: A\n",
        "model that performs better on the training data isn’t necessarily a model that will do\n",
        "better on data it has never seen before. \n",
        "\n",
        "In precise terms, what you’re seeing is overfitting : \n",
        "after the second or third epoch, we’re overoptimizing on the training data, and you end\n",
        "up learning representations that are specific to the training data and don’t generalize\n",
        "to data outside of the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCtYlB_NbyW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDm8hidVcsT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZzgtHOPctLD",
        "colab_type": "text"
      },
      "source": [
        "** So, we train a network from scratch for four epochs and then evaluate it on the\n",
        "test data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYz5z4PUcsO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiSS1Uu-YHIt",
        "colab_type": "code",
        "outputId": "86def6c8-73b4-48d7-f1e7-e7fc4308ee23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
        "results = model.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "25000/25000 [==============================] - 2s 79us/step - loss: 0.4578 - acc: 0.8230\n",
            "Epoch 2/4\n",
            "25000/25000 [==============================] - 2s 70us/step - loss: 0.2600 - acc: 0.9108\n",
            "Epoch 3/4\n",
            "25000/25000 [==============================] - 2s 71us/step - loss: 0.2032 - acc: 0.9261\n",
            "Epoch 4/4\n",
            "25000/25000 [==============================] - 2s 72us/step - loss: 0.1694 - acc: 0.9399\n",
            "25000/25000 [==============================] - 2s 96us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGbM528ZYGz3",
        "colab_type": "code",
        "outputId": "942bad24-e024-4c7a-cbe8-1a74a2a14268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3024928446483612, 0.88056]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrHiQO3KYiJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-_MQisudrbr",
        "colab_type": "code",
        "outputId": "a48150b5-adee-48a4-a8e9-58de59408ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(x_test[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 1. ... 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oVSifA1drqf",
        "colab_type": "code",
        "outputId": "c5420cd7-d42b-44ba-9d90-92efadaddeb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(y_test[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC8fnA5Ddruc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "some_variable = model.predict(x_test)  ### We can predict on new samples instead"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuKj8oykdrYG",
        "colab_type": "code",
        "outputId": "009edaa0-845f-4e89-84de-0dfaf807aecd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(type(some_variable))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0VcPW3QeWUq",
        "colab_type": "code",
        "outputId": "77d707a3-7e49-400a-d998-2cac404005e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "some_variable.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUnyjA7jfwUc",
        "colab_type": "code",
        "outputId": "fc3a7583-e674-40d1-b474-dea548b39680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "some_variable.ndim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw05RjcXeWQl",
        "colab_type": "code",
        "outputId": "7e79bc86-954c-4ae4-8907-93d318c7e58d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "some_variable[0,0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.21653476"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nKlIJQNeWL-",
        "colab_type": "code",
        "outputId": "04d05d58-a206-4372-c00f-9b565962edae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "some_variable[1,0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.99995637"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tpyEG5sfd5z",
        "colab_type": "code",
        "outputId": "45c2cf1b-90ad-4787-9ebe-c99727045905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(y_test[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBpsowqffd2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAc6-_lginMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeJi-O5EinFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV4b6Fq8in4H",
        "colab_type": "text"
      },
      "source": [
        "# How to experiment with different settings ? \n",
        "\n",
        "      - Exhaustive search over specified parameter values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_Si_V6Nfdyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Bxz20dYkXI",
        "colab_type": "code",
        "outputId": "3da7a907-d362-43a9-fec7-b53f0f624ca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.datasets import imdb\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
        "num_words=10000)\n",
        "\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1.\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')\n",
        "\n",
        "\n",
        "def dl_ex_classifier(optimizer):\n",
        "\tclassifier = models.Sequential()\n",
        "\tclassifier.add(Dense(units=16, kernel_initializer='uniform', activation='relu', input_dim=10000))\n",
        "\tclassifier.add(Dense(units=16, kernel_initializer='uniform', activation='relu'))\n",
        "\tclassifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
        " \n",
        "\tclassifier.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\treturn classifier\n",
        "\n",
        "\n",
        "classifier = KerasClassifier(build_fn=dl_ex_classifier)\n",
        "\n",
        "parameters = {'batch_size': [256, 512], 'epochs': [2, 4], 'optimizer': ['adam', 'rmsprop']}\n",
        "\n",
        "grid_search = GridSearchCV(estimator = classifier, param_grid = parameters, scoring = 'accuracy', cv =10)\n",
        "\n",
        "grid_search = grid_search.fit(x_train, y_train)\n",
        "\n",
        "best_parameters = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 5s 216us/step - loss: 0.6015 - acc: 0.7444\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 87us/step - loss: 0.2687 - acc: 0.8992\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 5s 218us/step - loss: 0.5797 - acc: 0.7920\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 88us/step - loss: 0.2561 - acc: 0.9030\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 5s 218us/step - loss: 0.5871 - acc: 0.7764\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 86us/step - loss: 0.2602 - acc: 0.9011\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 5s 220us/step - loss: 0.5898 - acc: 0.7146\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 87us/step - loss: 0.2602 - acc: 0.9034\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 5s 222us/step - loss: 0.5823 - acc: 0.7870\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 84us/step - loss: 0.2627 - acc: 0.9008\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 5s 212us/step - loss: 0.5986 - acc: 0.6993\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 84us/step - loss: 0.2619 - acc: 0.9021\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 5s 216us/step - loss: 0.5710 - acc: 0.7690\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 84us/step - loss: 0.2512 - acc: 0.9048\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 5s 234us/step - loss: 0.5723 - acc: 0.7871\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 88us/step - loss: 0.2548 - acc: 0.9043\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 5s 238us/step - loss: 0.5808 - acc: 0.7832\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 86us/step - loss: 0.2575 - acc: 0.9031\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 5s 240us/step - loss: 0.5901 - acc: 0.7133\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 88us/step - loss: 0.2565 - acc: 0.9025\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 5s 228us/step - loss: 0.5276 - acc: 0.8224\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 82us/step - loss: 0.2652 - acc: 0.9023\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 5s 224us/step - loss: 0.5308 - acc: 0.8173\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 81us/step - loss: 0.2668 - acc: 0.9009\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 5s 227us/step - loss: 0.5577 - acc: 0.7780\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 85us/step - loss: 0.2882 - acc: 0.8984\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 5s 230us/step - loss: 0.5329 - acc: 0.7955\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 83us/step - loss: 0.2701 - acc: 0.9007\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 5s 233us/step - loss: 0.5334 - acc: 0.8220\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 83us/step - loss: 0.2681 - acc: 0.9001\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 5s 239us/step - loss: 0.5293 - acc: 0.8223\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 83us/step - loss: 0.2684 - acc: 0.9022\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 5s 240us/step - loss: 0.5328 - acc: 0.7767\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 82us/step - loss: 0.2689 - acc: 0.9016\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 5s 241us/step - loss: 0.5255 - acc: 0.8225\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 84us/step - loss: 0.2659 - acc: 0.9026\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 5s 243us/step - loss: 0.5395 - acc: 0.7709\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 84us/step - loss: 0.2725 - acc: 0.9021\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 6s 248us/step - loss: 0.5532 - acc: 0.7868\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 83us/step - loss: 0.2817 - acc: 0.8982\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 6s 256us/step - loss: 0.5644 - acc: 0.7512\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 84us/step - loss: 0.2515 - acc: 0.9080\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 85us/step - loss: 0.1784 - acc: 0.9344\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 84us/step - loss: 0.1400 - acc: 0.9508\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 6s 264us/step - loss: 0.5863 - acc: 0.6923\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 87us/step - loss: 0.2636 - acc: 0.9051\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 88us/step - loss: 0.1819 - acc: 0.9351\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 90us/step - loss: 0.1429 - acc: 0.9512\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 6s 282us/step - loss: 0.5655 - acc: 0.7746\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 86us/step - loss: 0.2489 - acc: 0.9053\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 84us/step - loss: 0.1753 - acc: 0.9374\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 85us/step - loss: 0.1376 - acc: 0.9524\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 6s 269us/step - loss: 0.5791 - acc: 0.7717\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 86us/step - loss: 0.2588 - acc: 0.9034\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 85us/step - loss: 0.1813 - acc: 0.9334\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 84us/step - loss: 0.1425 - acc: 0.9502\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 6s 270us/step - loss: 0.5733 - acc: 0.7586\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 86us/step - loss: 0.2523 - acc: 0.9045\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 84us/step - loss: 0.1745 - acc: 0.9370\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 85us/step - loss: 0.1367 - acc: 0.9531\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 6s 271us/step - loss: 0.5758 - acc: 0.7970\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 84us/step - loss: 0.2592 - acc: 0.9033\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 85us/step - loss: 0.1798 - acc: 0.9348\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 85us/step - loss: 0.1418 - acc: 0.9512\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 6s 280us/step - loss: 0.5788 - acc: 0.7729\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 88us/step - loss: 0.2535 - acc: 0.9037\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 89us/step - loss: 0.1771 - acc: 0.9363\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 88us/step - loss: 0.1401 - acc: 0.9512\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 7s 300us/step - loss: 0.5914 - acc: 0.7230\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 88us/step - loss: 0.2602 - acc: 0.9023\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 87us/step - loss: 0.1846 - acc: 0.9332\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 85us/step - loss: 0.1452 - acc: 0.9498\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 6s 282us/step - loss: 0.5849 - acc: 0.7531\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 84us/step - loss: 0.2579 - acc: 0.9042\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 83us/step - loss: 0.1788 - acc: 0.9364\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 83us/step - loss: 0.1412 - acc: 0.9501\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 6s 284us/step - loss: 0.5646 - acc: 0.7893\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 84us/step - loss: 0.2530 - acc: 0.9044\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 83us/step - loss: 0.1783 - acc: 0.9354\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 84us/step - loss: 0.1394 - acc: 0.9521\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 6s 281us/step - loss: 0.5457 - acc: 0.8212\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 82us/step - loss: 0.2801 - acc: 0.8994\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 84us/step - loss: 0.2083 - acc: 0.9224\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 84us/step - loss: 0.1733 - acc: 0.9356\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 7s 306us/step - loss: 0.5218 - acc: 0.7877\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 86us/step - loss: 0.2609 - acc: 0.9031\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 88us/step - loss: 0.1976 - acc: 0.9278\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 87us/step - loss: 0.1654 - acc: 0.9402\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 7s 308us/step - loss: 0.5354 - acc: 0.8174\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 86us/step - loss: 0.2706 - acc: 0.9002\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 86us/step - loss: 0.2031 - acc: 0.9241\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 84us/step - loss: 0.1693 - acc: 0.9390\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 6s 288us/step - loss: 0.5433 - acc: 0.8140\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 82us/step - loss: 0.2750 - acc: 0.9008\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 81us/step - loss: 0.2029 - acc: 0.9248\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 82us/step - loss: 0.1703 - acc: 0.9389\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 6s 288us/step - loss: 0.5425 - acc: 0.7803\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 82us/step - loss: 0.2729 - acc: 0.9028\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 82us/step - loss: 0.2015 - acc: 0.9244\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 82us/step - loss: 0.1706 - acc: 0.9381\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 7s 297us/step - loss: 0.5328 - acc: 0.8142\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 82us/step - loss: 0.2698 - acc: 0.9013\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 83us/step - loss: 0.2010 - acc: 0.9254\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 85us/step - loss: 0.1687 - acc: 0.9390\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 7s 317us/step - loss: 0.5535 - acc: 0.8081\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 85us/step - loss: 0.2787 - acc: 0.8984\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 85us/step - loss: 0.2044 - acc: 0.9255\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 86us/step - loss: 0.1698 - acc: 0.9388\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 7s 322us/step - loss: 0.5360 - acc: 0.7872\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 83us/step - loss: 0.2703 - acc: 0.8993\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 83us/step - loss: 0.2027 - acc: 0.9242\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 87us/step - loss: 0.1704 - acc: 0.9388\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 7s 326us/step - loss: 0.5342 - acc: 0.8071\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 89us/step - loss: 0.2663 - acc: 0.9031\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 88us/step - loss: 0.1997 - acc: 0.9260\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 86us/step - loss: 0.1667 - acc: 0.9384\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 7s 312us/step - loss: 0.5370 - acc: 0.7684\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 86us/step - loss: 0.2662 - acc: 0.9027\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 85us/step - loss: 0.1947 - acc: 0.9295\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 85us/step - loss: 0.1631 - acc: 0.9418\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 7s 303us/step - loss: 0.6755 - acc: 0.6871\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 74us/step - loss: 0.4514 - acc: 0.8640\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 7s 307us/step - loss: 0.6699 - acc: 0.7405\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 74us/step - loss: 0.4209 - acc: 0.8690\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 7s 332us/step - loss: 0.6739 - acc: 0.6160\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 75us/step - loss: 0.4408 - acc: 0.8628\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 7s 330us/step - loss: 0.6744 - acc: 0.6478\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 75us/step - loss: 0.4712 - acc: 0.8540\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 8s 335us/step - loss: 0.6753 - acc: 0.7280\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 75us/step - loss: 0.4669 - acc: 0.8622\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 7s 328us/step - loss: 0.6764 - acc: 0.6668\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 73us/step - loss: 0.4568 - acc: 0.8576\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 7s 321us/step - loss: 0.6704 - acc: 0.5960\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 72us/step - loss: 0.4434 - acc: 0.8524\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 7s 325us/step - loss: 0.6750 - acc: 0.6293\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 72us/step - loss: 0.4561 - acc: 0.8545\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 7s 333us/step - loss: 0.6703 - acc: 0.6654\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 72us/step - loss: 0.4391 - acc: 0.8680\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 8s 352us/step - loss: 0.6698 - acc: 0.7392\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 75us/step - loss: 0.4295 - acc: 0.8708\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 8s 350us/step - loss: 0.6273 - acc: 0.7824\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 75us/step - loss: 0.3959 - acc: 0.8782\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 8s 353us/step - loss: 0.6230 - acc: 0.7905\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 75us/step - loss: 0.3874 - acc: 0.8834\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 8s 356us/step - loss: 0.6382 - acc: 0.7935\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 76us/step - loss: 0.4175 - acc: 0.8753\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 8s 354us/step - loss: 0.6394 - acc: 0.7472\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 74us/step - loss: 0.4344 - acc: 0.8741\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 8s 359us/step - loss: 0.6406 - acc: 0.6962\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 73us/step - loss: 0.4411 - acc: 0.8675\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 8s 354us/step - loss: 0.6249 - acc: 0.7347\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 74us/step - loss: 0.3888 - acc: 0.8832\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 8s 369us/step - loss: 0.6352 - acc: 0.6543\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 72us/step - loss: 0.4338 - acc: 0.8644\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 8s 372us/step - loss: 0.6273 - acc: 0.7965\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 74us/step - loss: 0.3976 - acc: 0.8798\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 8s 374us/step - loss: 0.6140 - acc: 0.7548\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 74us/step - loss: 0.3783 - acc: 0.8799\n",
            "Epoch 1/2\n",
            "22500/22500 [==============================] - 8s 375us/step - loss: 0.6132 - acc: 0.7832\n",
            "Epoch 2/2\n",
            "22500/22500 [==============================] - 2s 72us/step - loss: 0.3733 - acc: 0.8822\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 8s 374us/step - loss: 0.6727 - acc: 0.7316\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 75us/step - loss: 0.4484 - acc: 0.8665\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 73us/step - loss: 0.2541 - acc: 0.9085\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 73us/step - loss: 0.1914 - acc: 0.9311\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 8s 371us/step - loss: 0.6752 - acc: 0.6336\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 72us/step - loss: 0.4522 - acc: 0.8550\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 71us/step - loss: 0.2502 - acc: 0.9090\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 73us/step - loss: 0.1876 - acc: 0.9313\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 9s 382us/step - loss: 0.6709 - acc: 0.6332\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 76us/step - loss: 0.4598 - acc: 0.8472\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 75us/step - loss: 0.2569 - acc: 0.9101\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 76us/step - loss: 0.1903 - acc: 0.9327\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 9s 395us/step - loss: 0.6722 - acc: 0.7362\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 77us/step - loss: 0.4312 - acc: 0.8681\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 77us/step - loss: 0.2463 - acc: 0.9093\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 77us/step - loss: 0.1875 - acc: 0.9326\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 9s 403us/step - loss: 0.6742 - acc: 0.7463\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 77us/step - loss: 0.4471 - acc: 0.8673\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 78us/step - loss: 0.2519 - acc: 0.9071\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 78us/step - loss: 0.1902 - acc: 0.9308\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 9s 406us/step - loss: 0.6722 - acc: 0.6864\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 76us/step - loss: 0.4511 - acc: 0.8613\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 75us/step - loss: 0.2584 - acc: 0.9046\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 76us/step - loss: 0.1927 - acc: 0.9300\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 9s 412us/step - loss: 0.6740 - acc: 0.6312\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 77us/step - loss: 0.4408 - acc: 0.8629\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 77us/step - loss: 0.2479 - acc: 0.9090\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 76us/step - loss: 0.1871 - acc: 0.9326\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 9s 412us/step - loss: 0.6725 - acc: 0.6532\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 76us/step - loss: 0.4434 - acc: 0.8634\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 75us/step - loss: 0.2487 - acc: 0.9090\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 75us/step - loss: 0.1873 - acc: 0.9332\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 9s 419us/step - loss: 0.6678 - acc: 0.7327\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 76us/step - loss: 0.4306 - acc: 0.8638\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 77us/step - loss: 0.2476 - acc: 0.9108\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 75us/step - loss: 0.1891 - acc: 0.9324\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 9s 420us/step - loss: 0.6712 - acc: 0.7452\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 76us/step - loss: 0.4298 - acc: 0.8682\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 76us/step - loss: 0.2469 - acc: 0.9082\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 77us/step - loss: 0.1880 - acc: 0.9328\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 9s 420us/step - loss: 0.6293 - acc: 0.7712\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 75us/step - loss: 0.3966 - acc: 0.8783\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 74us/step - loss: 0.2638 - acc: 0.9052\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 74us/step - loss: 0.2087 - acc: 0.9233\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 10s 424us/step - loss: 0.6206 - acc: 0.7418\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 74us/step - loss: 0.3893 - acc: 0.8804\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 76us/step - loss: 0.2608 - acc: 0.9091\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 75us/step - loss: 0.2081 - acc: 0.9253\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 10s 426us/step - loss: 0.6205 - acc: 0.7789\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 75us/step - loss: 0.3840 - acc: 0.8809\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 72us/step - loss: 0.2595 - acc: 0.9065\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 75us/step - loss: 0.2070 - acc: 0.9242\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 10s 430us/step - loss: 0.6207 - acc: 0.7780\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 74us/step - loss: 0.3854 - acc: 0.8807\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 75us/step - loss: 0.2608 - acc: 0.9081\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 74us/step - loss: 0.2091 - acc: 0.9231\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 10s 423us/step - loss: 0.6203 - acc: 0.7897\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 70us/step - loss: 0.3810 - acc: 0.8803\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 71us/step - loss: 0.2543 - acc: 0.9082\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 71us/step - loss: 0.2033 - acc: 0.9252\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 9s 413us/step - loss: 0.6147 - acc: 0.7523\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 70us/step - loss: 0.3804 - acc: 0.8800\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 71us/step - loss: 0.2579 - acc: 0.9088\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 69us/step - loss: 0.2049 - acc: 0.9259\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 9s 420us/step - loss: 0.6205 - acc: 0.6967\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 71us/step - loss: 0.3901 - acc: 0.8829\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 71us/step - loss: 0.2587 - acc: 0.9105\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 71us/step - loss: 0.2051 - acc: 0.9278\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 9s 417us/step - loss: 0.6159 - acc: 0.7795\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 70us/step - loss: 0.3860 - acc: 0.8789\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 70us/step - loss: 0.2623 - acc: 0.9069\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 70us/step - loss: 0.2100 - acc: 0.9236\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 9s 421us/step - loss: 0.6061 - acc: 0.7994\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 70us/step - loss: 0.3655 - acc: 0.8800\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 71us/step - loss: 0.2506 - acc: 0.9078\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 70us/step - loss: 0.2025 - acc: 0.9259\n",
            "Epoch 1/4\n",
            "22500/22500 [==============================] - 10s 425us/step - loss: 0.6194 - acc: 0.7609\n",
            "Epoch 2/4\n",
            "22500/22500 [==============================] - 2s 70us/step - loss: 0.3802 - acc: 0.8811\n",
            "Epoch 3/4\n",
            "22500/22500 [==============================] - 2s 71us/step - loss: 0.2563 - acc: 0.9082\n",
            "Epoch 4/4\n",
            "22500/22500 [==============================] - 2s 70us/step - loss: 0.2037 - acc: 0.9250\n",
            "Epoch 1/4\n",
            "25000/25000 [==============================] - 10s 389us/step - loss: 0.6019 - acc: 0.7717\n",
            "Epoch 2/4\n",
            "25000/25000 [==============================] - 2s 69us/step - loss: 0.3471 - acc: 0.8864\n",
            "Epoch 3/4\n",
            "25000/25000 [==============================] - 2s 71us/step - loss: 0.2391 - acc: 0.9139\n",
            "Epoch 4/4\n",
            "25000/25000 [==============================] - 2s 69us/step - loss: 0.1978 - acc: 0.9276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaHY_6fxYkdI",
        "colab_type": "code",
        "outputId": "f833f882-548d-423e-e20a-aa648685a623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "best_parameters"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 512, 'epochs': 4, 'optimizer': 'rmsprop'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUB5Bjq6YkbJ",
        "colab_type": "code",
        "outputId": "b7d036cb-da0e-4d55-d508-dd039a528ee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "best_accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.89512"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wexa0hr0YkUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2fWcXkOtEap",
        "colab_type": "text"
      },
      "source": [
        "# Default Cross validation value = 3 (5 in updated versions)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg6Ne8uDtJj8",
        "colab_type": "code",
        "outputId": "42722522-8974-495b-9372-62423e9edb85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.datasets import imdb\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
        "num_words=10000)\n",
        "\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1.\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')\n",
        "\n",
        "\n",
        "def dl_ex_classifier(optimizer):\n",
        "\tclassifier = models.Sequential()\n",
        "\tclassifier.add(Dense(units=16, kernel_initializer='uniform', activation='relu', input_dim=10000))\n",
        "\tclassifier.add(Dense(units=16, kernel_initializer='uniform', activation='relu'))\n",
        "\tclassifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
        " \n",
        "\tclassifier.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\treturn classifier\n",
        "\n",
        "\n",
        "classifier = KerasClassifier(build_fn=dl_ex_classifier)\n",
        "\n",
        "parameters = {'batch_size': [256, 512], 'epochs': [2, 4], 'optimizer': ['adam', 'rmsprop']}\n",
        "\n",
        "grid_search = GridSearchCV(estimator = classifier, param_grid = parameters, scoring = 'accuracy')\n",
        "\n",
        "grid_search = grid_search.fit(x_train, y_train)\n",
        "\n",
        "best_parameters = grid_search.best_params_technology forecasting using patent analysis \n",
        "best_accuracy = grid_search.best_score_\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "16666/16666 [==============================] - 10s 578us/step - loss: 0.6445 - acc: 0.7373\n",
            "Epoch 2/2\n",
            "16666/16666 [==============================] - 1s 86us/step - loss: 0.3321 - acc: 0.8879\n",
            "Epoch 1/2\n",
            "16667/16667 [==============================] - 10s 579us/step - loss: 0.6342 - acc: 0.7434\n",
            "Epoch 2/2\n",
            "16667/16667 [==============================] - 1s 84us/step - loss: 0.3125 - acc: 0.8886\n",
            "Epoch 1/2\n",
            "16667/16667 [==============================] - 10s 583us/step - loss: 0.6478 - acc: 0.6440\n",
            "Epoch 2/2\n",
            "16667/16667 [==============================] - 1s 85us/step - loss: 0.3401 - acc: 0.8853\n",
            "Epoch 1/2\n",
            "16666/16666 [==============================] - 10s 578us/step - loss: 0.5832 - acc: 0.8007\n",
            "Epoch 2/2\n",
            "16666/16666 [==============================] - 1s 85us/step - loss: 0.3104 - acc: 0.8950\n",
            "Epoch 1/2\n",
            "16667/16667 [==============================] - 10s 591us/step - loss: 0.5867 - acc: 0.7782\n",
            "Epoch 2/2\n",
            "16667/16667 [==============================] - 1s 82us/step - loss: 0.3130 - acc: 0.8950\n",
            "Epoch 1/2\n",
            "16667/16667 [==============================] - 10s 585us/step - loss: 0.5961 - acc: 0.7795\n",
            "Epoch 2/2\n",
            "16667/16667 [==============================] - 1s 82us/step - loss: 0.3250 - acc: 0.8905\n",
            "Epoch 1/4\n",
            "16666/16666 [==============================] - 10s 590us/step - loss: 0.6567 - acc: 0.5965\n",
            "Epoch 2/4\n",
            "16666/16666 [==============================] - 1s 84us/step - loss: 0.3570 - acc: 0.8774\n",
            "Epoch 3/4\n",
            "16666/16666 [==============================] - 1s 83us/step - loss: 0.2048 - acc: 0.9256\n",
            "Epoch 4/4\n",
            "16666/16666 [==============================] - 1s 85us/step - loss: 0.1505 - acc: 0.9487\n",
            "Epoch 1/4\n",
            "16667/16667 [==============================] - 10s 597us/step - loss: 0.6425 - acc: 0.7432\n",
            "Epoch 2/4\n",
            "16667/16667 [==============================] - 1s 85us/step - loss: 0.3221 - acc: 0.8903\n",
            "Epoch 3/4\n",
            "16667/16667 [==============================] - 1s 84us/step - loss: 0.1940 - acc: 0.9313\n",
            "Epoch 4/4\n",
            "16667/16667 [==============================] - 1s 84us/step - loss: 0.1444 - acc: 0.9525\n",
            "Epoch 1/4\n",
            "16667/16667 [==============================] - 10s 600us/step - loss: 0.6343 - acc: 0.6702\n",
            "Epoch 2/4\n",
            "16667/16667 [==============================] - 1s 83us/step - loss: 0.3166 - acc: 0.8893\n",
            "Epoch 3/4\n",
            "16667/16667 [==============================] - 1s 84us/step - loss: 0.1947 - acc: 0.9301\n",
            "Epoch 4/4\n",
            "16667/16667 [==============================] - 1s 83us/step - loss: 0.1455 - acc: 0.9503\n",
            "Epoch 1/4\n",
            "16666/16666 [==============================] - 10s 599us/step - loss: 0.5794 - acc: 0.7848\n",
            "Epoch 2/4\n",
            "16666/16666 [==============================] - 1s 82us/step - loss: 0.3108 - acc: 0.8927\n",
            "Epoch 3/4\n",
            "16666/16666 [==============================] - 1s 82us/step - loss: 0.2157 - acc: 0.9212\n",
            "Epoch 4/4\n",
            "16666/16666 [==============================] - 1s 80us/step - loss: 0.1723 - acc: 0.9386\n",
            "Epoch 1/4\n",
            "16667/16667 [==============================] - 10s 598us/step - loss: 0.5900 - acc: 0.7778\n",
            "Epoch 2/4\n",
            "16667/16667 [==============================] - 1s 81us/step - loss: 0.3219 - acc: 0.8900\n",
            "Epoch 3/4\n",
            "16667/16667 [==============================] - 1s 82us/step - loss: 0.2228 - acc: 0.9179\n",
            "Epoch 4/4\n",
            "16667/16667 [==============================] - 1s 83us/step - loss: 0.1774 - acc: 0.9359\n",
            "Epoch 1/4\n",
            "16667/16667 [==============================] - 10s 605us/step - loss: 0.5926 - acc: 0.7368\n",
            "Epoch 2/4\n",
            "16667/16667 [==============================] - 1s 83us/step - loss: 0.3204 - acc: 0.8938\n",
            "Epoch 3/4\n",
            "16667/16667 [==============================] - 1s 81us/step - loss: 0.2158 - acc: 0.9222\n",
            "Epoch 4/4\n",
            "16667/16667 [==============================] - 1s 80us/step - loss: 0.1711 - acc: 0.9387\n",
            "Epoch 1/2\n",
            "16666/16666 [==============================] - 10s 605us/step - loss: 0.6869 - acc: 0.5966\n",
            "Epoch 2/2\n",
            "16666/16666 [==============================] - 1s 70us/step - loss: 0.5715 - acc: 0.8465\n",
            "Epoch 1/2\n",
            "16667/16667 [==============================] - 10s 603us/step - loss: 0.6854 - acc: 0.6020\n",
            "Epoch 2/2\n",
            "16667/16667 [==============================] - 1s 70us/step - loss: 0.5580 - acc: 0.8352\n",
            "Epoch 1/2\n",
            "16667/16667 [==============================] - 10s 614us/step - loss: 0.6831 - acc: 0.5889\n",
            "Epoch 2/2\n",
            "16667/16667 [==============================] - 1s 69us/step - loss: 0.5607 - acc: 0.8172\n",
            "Epoch 1/2\n",
            "16666/16666 [==============================] - 10s 622us/step - loss: 0.6407 - acc: 0.7719\n",
            "Epoch 2/2\n",
            "16666/16666 [==============================] - 1s 70us/step - loss: 0.4383 - acc: 0.8736\n",
            "Epoch 1/2\n",
            "16667/16667 [==============================] - 10s 620us/step - loss: 0.6466 - acc: 0.7283\n",
            "Epoch 2/2\n",
            "16667/16667 [==============================] - 1s 70us/step - loss: 0.4519 - acc: 0.8719\n",
            "Epoch 1/2\n",
            "16667/16667 [==============================] - 10s 621us/step - loss: 0.6577 - acc: 0.7571\n",
            "Epoch 2/2\n",
            "16667/16667 [==============================] - 1s 69us/step - loss: 0.4757 - acc: 0.8690\n",
            "Epoch 1/4\n",
            "16666/16666 [==============================] - 10s 627us/step - loss: 0.6844 - acc: 0.5612\n",
            "Epoch 2/4\n",
            "16666/16666 [==============================] - 1s 71us/step - loss: 0.5587 - acc: 0.8151\n",
            "Epoch 3/4\n",
            "16666/16666 [==============================] - 1s 71us/step - loss: 0.3252 - acc: 0.8930\n",
            "Epoch 4/4\n",
            "16666/16666 [==============================] - 1s 71us/step - loss: 0.2160 - acc: 0.9240\n",
            "Epoch 1/4\n",
            "16667/16667 [==============================] - 11s 630us/step - loss: 0.6852 - acc: 0.5943\n",
            "Epoch 2/4\n",
            "16667/16667 [==============================] - 1s 72us/step - loss: 0.5605 - acc: 0.8439\n",
            "Epoch 3/4\n",
            "16667/16667 [==============================] - 1s 71us/step - loss: 0.3248 - acc: 0.8928\n",
            "Epoch 4/4\n",
            "16667/16667 [==============================] - 1s 71us/step - loss: 0.2179 - acc: 0.9228\n",
            "Epoch 1/4\n",
            "16667/16667 [==============================] - 11s 639us/step - loss: 0.6849 - acc: 0.7008\n",
            "Epoch 2/4\n",
            "16667/16667 [==============================] - 1s 70us/step - loss: 0.5542 - acc: 0.8497\n",
            "Epoch 3/4\n",
            "16667/16667 [==============================] - 1s 70us/step - loss: 0.3255 - acc: 0.8916\n",
            "Epoch 4/4\n",
            "16667/16667 [==============================] - 1s 71us/step - loss: 0.2222 - acc: 0.9215\n",
            "Epoch 1/4\n",
            "16666/16666 [==============================] - 11s 637us/step - loss: 0.6503 - acc: 0.7671\n",
            "Epoch 2/4\n",
            "16666/16666 [==============================] - 1s 70us/step - loss: 0.4556 - acc: 0.8721\n",
            "Epoch 3/4\n",
            "16666/16666 [==============================] - 1s 70us/step - loss: 0.3052 - acc: 0.8993\n",
            "Epoch 4/4\n",
            "16666/16666 [==============================] - 1s 70us/step - loss: 0.2301 - acc: 0.9185\n",
            "Epoch 1/4\n",
            "16667/16667 [==============================] - 11s 642us/step - loss: 0.6532 - acc: 0.7571\n",
            "Epoch 2/4\n",
            "16667/16667 [==============================] - 1s 70us/step - loss: 0.4670 - acc: 0.8714\n",
            "Epoch 3/4\n",
            "16667/16667 [==============================] - 1s 70us/step - loss: 0.3172 - acc: 0.8988\n",
            "Epoch 4/4\n",
            "16667/16667 [==============================] - 1s 69us/step - loss: 0.2367 - acc: 0.9197\n",
            "Epoch 1/4\n",
            "16667/16667 [==============================] - 11s 648us/step - loss: 0.6403 - acc: 0.7674\n",
            "Epoch 2/4\n",
            "16667/16667 [==============================] - 1s 69us/step - loss: 0.4378 - acc: 0.8749\n",
            "Epoch 3/4\n",
            "16667/16667 [==============================] - 1s 70us/step - loss: 0.2956 - acc: 0.9015\n",
            "Epoch 4/4\n",
            "16667/16667 [==============================] - 1s 71us/step - loss: 0.2260 - acc: 0.9200\n",
            "Epoch 1/4\n",
            "25000/25000 [==============================] - 11s 458us/step - loss: 0.6642 - acc: 0.7010\n",
            "Epoch 2/4\n",
            "25000/25000 [==============================] - 2s 71us/step - loss: 0.4157 - acc: 0.8664\n",
            "Epoch 3/4\n",
            "25000/25000 [==============================] - 2s 70us/step - loss: 0.2415 - acc: 0.9137\n",
            "Epoch 4/4\n",
            "25000/25000 [==============================] - 2s 71us/step - loss: 0.1859 - acc: 0.9338\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy1pR7_3EJpm",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lLHIXBgtS_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2NNaOzutASc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpALwtoJw-xx",
        "colab_type": "text"
      },
      "source": [
        "# 2. Classify Reuters newswires into 46 mutually exclusive topics\n",
        "\n",
        "\n",
        "- Data: Use Reuters data from keras \n",
        "- each data point should be classified into only one category, the problem is more specifically an instance of single-label, multiclass classification\n",
        "- Build model\n",
        "- Train and Validate\n",
        "- Check for overfit case if any: \n",
        "- Plot the results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UObvUAEhtAWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWKnLaRZxHzS",
        "colab_type": "text"
      },
      "source": [
        "# Use very few words( top 1000) and small number of epochs =5 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-aHkg7ttAqz",
        "colab_type": "code",
        "outputId": "28663186-a30e-4e87-e6b0-f5f2b4e7c9e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        }
      },
      "source": [
        "'''Trains and evaluate a simple MLP\n",
        "on the Reuters newswire topic classification task.\n",
        "'''\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import reuters\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "max_words = 1000 \n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
        "                                                         test_split=0.2)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "num_classes = np.max(y_train) + 1\n",
        "print(num_classes, 'classes')\n",
        "\n",
        "print('Vectorizing sequence data...')\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Convert class vector to binary class matrix '\n",
        "      '(for use with categorical_crossentropy)')\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "\n",
        "print('Building model...')\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)\n",
        "score = model.evaluate(x_test, y_test,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 1s 1us/step\n",
            "8982 train sequences\n",
            "2246 test sequences\n",
            "46 classes\n",
            "Vectorizing sequence data...\n",
            "x_train shape: (8982, 1000)\n",
            "x_test shape: (2246, 1000)\n",
            "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
            "y_train shape: (8982, 46)\n",
            "y_test shape: (2246, 46)\n",
            "Building model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Train on 8083 samples, validate on 899 samples\n",
            "Epoch 1/5\n",
            "8083/8083 [==============================] - 2s 213us/step - loss: 1.4266 - acc: 0.6811 - val_loss: 1.0780 - val_acc: 0.7653\n",
            "Epoch 2/5\n",
            "8083/8083 [==============================] - 1s 172us/step - loss: 0.7941 - acc: 0.8122 - val_loss: 0.9091 - val_acc: 0.7898\n",
            "Epoch 3/5\n",
            "8083/8083 [==============================] - 1s 174us/step - loss: 0.5474 - acc: 0.8711 - val_loss: 0.8766 - val_acc: 0.7998\n",
            "Epoch 4/5\n",
            "8083/8083 [==============================] - 1s 171us/step - loss: 0.4180 - acc: 0.8932 - val_loss: 0.8613 - val_acc: 0.7998\n",
            "Epoch 5/5\n",
            "8083/8083 [==============================] - 1s 176us/step - loss: 0.3223 - acc: 0.9177 - val_loss: 0.8885 - val_acc: 0.8098\n",
            "2246/2246 [==============================] - 0s 56us/step\n",
            "Test score: 0.8552183270560559\n",
            "Test accuracy: 0.7965271594475554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU3e40BZtAQ8",
        "colab_type": "code",
        "outputId": "3fbf3c3f-69ee-4a9c-a314-975964ac1aa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "score "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8552183270560559, 0.7965271594475554]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B11MUbVqtAMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11-DEHiAwHEm",
        "colab_type": "text"
      },
      "source": [
        "# Just changing the number of epochs, does it make any difference in terms of loss and accuracy ? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdQq5PnZwR-1",
        "colab_type": "code",
        "outputId": "8df06602-5c54-4edd-d96f-874a8acde5c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''Trains and evaluate a simple MLP\n",
        "on the Reuters newswire topic classification task.\n",
        "'''\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import reuters\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "max_words = 1000\n",
        "batch_size = 32\n",
        "epochs = 25\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
        "                                                         test_split=0.2)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "num_classes = np.max(y_train) + 1\n",
        "print(num_classes, 'classes')\n",
        "\n",
        "print('Vectorizing sequence data...')\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Convert class vector to binary class matrix '\n",
        "      '(for use with categorical_crossentropy)')\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "\n",
        "print('Building model...')\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)\n",
        "score = model.evaluate(x_test, y_test,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "8982 train sequences\n",
            "2246 test sequences\n",
            "46 classes\n",
            "Vectorizing sequence data...\n",
            "x_train shape: (8982, 1000)\n",
            "x_test shape: (2246, 1000)\n",
            "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
            "y_train shape: (8982, 46)\n",
            "y_test shape: (2246, 46)\n",
            "Building model...\n",
            "Train on 8083 samples, validate on 899 samples\n",
            "Epoch 1/25\n",
            "8083/8083 [==============================] - 2s 211us/step - loss: 1.4107 - acc: 0.6816 - val_loss: 1.0627 - val_acc: 0.7631\n",
            "Epoch 2/25\n",
            "8083/8083 [==============================] - 1s 171us/step - loss: 0.7915 - acc: 0.8152 - val_loss: 0.9175 - val_acc: 0.8053\n",
            "Epoch 3/25\n",
            "8083/8083 [==============================] - 1s 171us/step - loss: 0.5483 - acc: 0.8656 - val_loss: 0.8686 - val_acc: 0.7964\n",
            "Epoch 4/25\n",
            "8083/8083 [==============================] - 1s 177us/step - loss: 0.4121 - acc: 0.8988 - val_loss: 0.8427 - val_acc: 0.8142\n",
            "Epoch 5/25\n",
            "8083/8083 [==============================] - 1s 169us/step - loss: 0.3369 - acc: 0.9161 - val_loss: 0.8890 - val_acc: 0.8087\n",
            "Epoch 6/25\n",
            "8083/8083 [==============================] - 1s 173us/step - loss: 0.2706 - acc: 0.9303 - val_loss: 0.9076 - val_acc: 0.8131\n",
            "Epoch 7/25\n",
            "8083/8083 [==============================] - 1s 177us/step - loss: 0.2314 - acc: 0.9380 - val_loss: 0.9350 - val_acc: 0.8087\n",
            "Epoch 8/25\n",
            "8083/8083 [==============================] - 1s 175us/step - loss: 0.2107 - acc: 0.9442 - val_loss: 0.9486 - val_acc: 0.8042\n",
            "Epoch 9/25\n",
            "8083/8083 [==============================] - 1s 169us/step - loss: 0.1933 - acc: 0.9458 - val_loss: 1.0161 - val_acc: 0.7942\n",
            "Epoch 10/25\n",
            "8083/8083 [==============================] - 1s 177us/step - loss: 0.1836 - acc: 0.9510 - val_loss: 1.0078 - val_acc: 0.7976\n",
            "Epoch 11/25\n",
            "8083/8083 [==============================] - 1s 178us/step - loss: 0.1734 - acc: 0.9518 - val_loss: 1.0506 - val_acc: 0.7976\n",
            "Epoch 12/25\n",
            "8083/8083 [==============================] - 1s 180us/step - loss: 0.1689 - acc: 0.9504 - val_loss: 1.0479 - val_acc: 0.7898\n",
            "Epoch 13/25\n",
            "8083/8083 [==============================] - 1s 175us/step - loss: 0.1726 - acc: 0.9539 - val_loss: 1.0527 - val_acc: 0.8020\n",
            "Epoch 14/25\n",
            "8083/8083 [==============================] - 1s 173us/step - loss: 0.1605 - acc: 0.9557 - val_loss: 1.0677 - val_acc: 0.7931\n",
            "Epoch 15/25\n",
            "8083/8083 [==============================] - 1s 178us/step - loss: 0.1601 - acc: 0.9542 - val_loss: 1.1309 - val_acc: 0.7898\n",
            "Epoch 16/25\n",
            "8083/8083 [==============================] - 1s 173us/step - loss: 0.1603 - acc: 0.9537 - val_loss: 1.1208 - val_acc: 0.7942\n",
            "Epoch 17/25\n",
            "8083/8083 [==============================] - 1s 172us/step - loss: 0.1533 - acc: 0.9555 - val_loss: 1.1168 - val_acc: 0.7976\n",
            "Epoch 18/25\n",
            "8083/8083 [==============================] - 1s 173us/step - loss: 0.1454 - acc: 0.9584 - val_loss: 1.1793 - val_acc: 0.7920\n",
            "Epoch 19/25\n",
            "8083/8083 [==============================] - 1s 176us/step - loss: 0.1462 - acc: 0.9565 - val_loss: 1.1406 - val_acc: 0.7864\n",
            "Epoch 20/25\n",
            "8083/8083 [==============================] - 1s 182us/step - loss: 0.1535 - acc: 0.9577 - val_loss: 1.1578 - val_acc: 0.7909\n",
            "Epoch 21/25\n",
            "8083/8083 [==============================] - 1s 181us/step - loss: 0.1469 - acc: 0.9576 - val_loss: 1.1451 - val_acc: 0.7942\n",
            "Epoch 22/25\n",
            "8083/8083 [==============================] - 1s 181us/step - loss: 0.1521 - acc: 0.9593 - val_loss: 1.2075 - val_acc: 0.7831\n",
            "Epoch 23/25\n",
            "8083/8083 [==============================] - 1s 185us/step - loss: 0.1451 - acc: 0.9578 - val_loss: 1.1620 - val_acc: 0.7953\n",
            "Epoch 24/25\n",
            "8083/8083 [==============================] - 1s 181us/step - loss: 0.1432 - acc: 0.9582 - val_loss: 1.1703 - val_acc: 0.7942\n",
            "Epoch 25/25\n",
            "8083/8083 [==============================] - 1s 183us/step - loss: 0.1443 - acc: 0.9589 - val_loss: 1.1755 - val_acc: 0.7953\n",
            "2246/2246 [==============================] - 0s 62us/step\n",
            "Test score: 1.1806842762767897\n",
            "Test accuracy: 0.7880676759212865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGLmQyRtwU57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFgWZRtgxTui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYrIF3osxTy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etgCLBnIxU9l",
        "colab_type": "text"
      },
      "source": [
        "# Changing the value top words to considered from 1000 to 10000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49iV_1vGxTry",
        "colab_type": "code",
        "outputId": "ac94ded6-cf9d-41e0-aee7-c48ab5489220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''Trains and evaluate a simple MLP\n",
        "on the Reuters newswire topic classification task.\n",
        "'''\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import reuters\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "max_words = 10000\n",
        "batch_size = 32\n",
        "epochs = 25\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
        "                                                         test_split=0.2)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "num_classes = np.max(y_train) + 1\n",
        "print(num_classes, 'classes')\n",
        "\n",
        "print('Vectorizing sequence data...')\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Convert class vector to binary class matrix '\n",
        "      '(for use with categorical_crossentropy)')\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "\n",
        "print('Building model...')\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)\n",
        "score = model.evaluate(x_test, y_test,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "8982 train sequences\n",
            "2246 test sequences\n",
            "46 classes\n",
            "Vectorizing sequence data...\n",
            "x_train shape: (8982, 10000)\n",
            "x_test shape: (2246, 10000)\n",
            "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
            "y_train shape: (8982, 46)\n",
            "y_test shape: (2246, 46)\n",
            "Building model...\n",
            "Train on 8083 samples, validate on 899 samples\n",
            "Epoch 1/25\n",
            "8083/8083 [==============================] - 4s 440us/step - loss: 1.3102 - acc: 0.7169 - val_loss: 0.9452 - val_acc: 0.7998\n",
            "Epoch 2/25\n",
            "8083/8083 [==============================] - 3s 372us/step - loss: 0.4979 - acc: 0.8864 - val_loss: 0.8598 - val_acc: 0.8120\n",
            "Epoch 3/25\n",
            "8083/8083 [==============================] - 3s 382us/step - loss: 0.2801 - acc: 0.9362 - val_loss: 0.9026 - val_acc: 0.8187\n",
            "Epoch 4/25\n",
            "8083/8083 [==============================] - 3s 378us/step - loss: 0.2114 - acc: 0.9493 - val_loss: 0.9545 - val_acc: 0.8176\n",
            "Epoch 5/25\n",
            "8083/8083 [==============================] - 3s 382us/step - loss: 0.1980 - acc: 0.9531 - val_loss: 0.9913 - val_acc: 0.8120\n",
            "Epoch 6/25\n",
            "8083/8083 [==============================] - 3s 377us/step - loss: 0.1801 - acc: 0.9556 - val_loss: 1.0396 - val_acc: 0.8076\n",
            "Epoch 7/25\n",
            "8083/8083 [==============================] - 3s 376us/step - loss: 0.1850 - acc: 0.9555 - val_loss: 1.0532 - val_acc: 0.8009\n",
            "Epoch 8/25\n",
            "8083/8083 [==============================] - 3s 377us/step - loss: 0.1667 - acc: 0.9605 - val_loss: 1.1180 - val_acc: 0.7976\n",
            "Epoch 9/25\n",
            "8083/8083 [==============================] - 3s 373us/step - loss: 0.1896 - acc: 0.9569 - val_loss: 1.0553 - val_acc: 0.8053\n",
            "Epoch 10/25\n",
            "8083/8083 [==============================] - 3s 376us/step - loss: 0.1718 - acc: 0.9588 - val_loss: 1.1313 - val_acc: 0.8009\n",
            "Epoch 11/25\n",
            "8083/8083 [==============================] - 3s 381us/step - loss: 0.1731 - acc: 0.9584 - val_loss: 1.1279 - val_acc: 0.8053\n",
            "Epoch 12/25\n",
            "8083/8083 [==============================] - 3s 375us/step - loss: 0.1837 - acc: 0.9576 - val_loss: 1.1421 - val_acc: 0.8020\n",
            "Epoch 13/25\n",
            "8083/8083 [==============================] - 3s 375us/step - loss: 0.1635 - acc: 0.9603 - val_loss: 1.1305 - val_acc: 0.8020\n",
            "Epoch 14/25\n",
            "8083/8083 [==============================] - 3s 374us/step - loss: 0.1816 - acc: 0.9582 - val_loss: 1.1202 - val_acc: 0.8020\n",
            "Epoch 15/25\n",
            "8083/8083 [==============================] - 3s 378us/step - loss: 0.1699 - acc: 0.9602 - val_loss: 1.2334 - val_acc: 0.7998\n",
            "Epoch 16/25\n",
            "8083/8083 [==============================] - 3s 376us/step - loss: 0.1742 - acc: 0.9592 - val_loss: 1.1744 - val_acc: 0.7998\n",
            "Epoch 17/25\n",
            "8083/8083 [==============================] - 3s 370us/step - loss: 0.1723 - acc: 0.9583 - val_loss: 1.2249 - val_acc: 0.8031\n",
            "Epoch 18/25\n",
            "8083/8083 [==============================] - 3s 375us/step - loss: 0.1590 - acc: 0.9623 - val_loss: 1.1947 - val_acc: 0.8065\n",
            "Epoch 19/25\n",
            "8083/8083 [==============================] - 3s 377us/step - loss: 0.1656 - acc: 0.9616 - val_loss: 1.2534 - val_acc: 0.7953\n",
            "Epoch 20/25\n",
            "8083/8083 [==============================] - 3s 372us/step - loss: 0.1602 - acc: 0.9597 - val_loss: 1.2438 - val_acc: 0.7998\n",
            "Epoch 21/25\n",
            "8083/8083 [==============================] - 3s 374us/step - loss: 0.1689 - acc: 0.9600 - val_loss: 1.2419 - val_acc: 0.7998\n",
            "Epoch 22/25\n",
            "8083/8083 [==============================] - 3s 375us/step - loss: 0.1649 - acc: 0.9595 - val_loss: 1.2930 - val_acc: 0.7976\n",
            "Epoch 23/25\n",
            "8083/8083 [==============================] - 3s 373us/step - loss: 0.1716 - acc: 0.9607 - val_loss: 1.3218 - val_acc: 0.7931\n",
            "Epoch 24/25\n",
            "8083/8083 [==============================] - 3s 371us/step - loss: 0.1557 - acc: 0.9615 - val_loss: 1.3731 - val_acc: 0.7976\n",
            "Epoch 25/25\n",
            "8083/8083 [==============================] - 3s 371us/step - loss: 0.1612 - acc: 0.9610 - val_loss: 1.3293 - val_acc: 0.7964\n",
            "2246/2246 [==============================] - 0s 98us/step\n",
            "Test score: 1.2723496372107406\n",
            "Test accuracy: 0.7938557435971546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y01cqwRfxlxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Tbhq_QEynuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulKhIoJMyoND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK2NJzgTysgN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# What is Information bottleneck ?? Can we check this with an example ???\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G-3Yk1XyntO",
        "colab_type": "code",
        "outputId": "8978b2da-6ddc-45cf-c0ab-990bfb38ee90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "'''Trains and evaluate a simple MLP\n",
        "on the Reuters newswire topic classification task.\n",
        "'''\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import reuters\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "max_words = 10000\n",
        "batch_size = 32\n",
        "epochs = 25\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
        "                                                         test_split=0.2)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "num_classes = np.max(y_train) + 1\n",
        "print(num_classes, 'classes')\n",
        "\n",
        "print('Vectorizing sequence data...')\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Convert class vector to binary class matrix '\n",
        "      '(for use with categorical_crossentropy)')\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "\n",
        "print('Building model...')\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(4, input_shape=(max_words,)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)\n",
        "score = model.evaluate(x_test, y_test,\n",
        "                       batch_size=batch_size, verbose=1)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "8982 train sequences\n",
            "2246 test sequences\n",
            "46 classes\n",
            "Vectorizing sequence data...\n",
            "x_train shape: (8982, 10000)\n",
            "x_test shape: (2246, 10000)\n",
            "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
            "y_train shape: (8982, 46)\n",
            "y_test shape: (2246, 46)\n",
            "Building model...\n",
            "Train on 8083 samples, validate on 899 samples\n",
            "Epoch 1/25\n",
            "8083/8083 [==============================] - 4s 474us/step - loss: 2.9818 - acc: 0.3016 - val_loss: 2.1142 - val_acc: 0.3782\n",
            "Epoch 2/25\n",
            "8083/8083 [==============================] - 3s 377us/step - loss: 2.3805 - acc: 0.4125 - val_loss: 1.7256 - val_acc: 0.6040\n",
            "Epoch 3/25\n",
            "8083/8083 [==============================] - 3s 380us/step - loss: 2.0399 - acc: 0.4443 - val_loss: 1.6896 - val_acc: 0.5907\n",
            "Epoch 4/25\n",
            "8083/8083 [==============================] - 3s 382us/step - loss: 1.9224 - acc: 0.4417 - val_loss: 1.6482 - val_acc: 0.6007\n",
            "Epoch 5/25\n",
            "8083/8083 [==============================] - 3s 380us/step - loss: 1.8554 - acc: 0.4482 - val_loss: 1.6216 - val_acc: 0.5984\n",
            "Epoch 6/25\n",
            "8083/8083 [==============================] - 3s 378us/step - loss: 1.8339 - acc: 0.4397 - val_loss: 1.6140 - val_acc: 0.5984\n",
            "Epoch 7/25\n",
            "8083/8083 [==============================] - 3s 373us/step - loss: 1.7830 - acc: 0.4469 - val_loss: 1.6243 - val_acc: 0.5929\n",
            "Epoch 8/25\n",
            "8083/8083 [==============================] - 3s 376us/step - loss: 1.7550 - acc: 0.4522 - val_loss: 1.6104 - val_acc: 0.5962\n",
            "Epoch 9/25\n",
            "8083/8083 [==============================] - 3s 380us/step - loss: 1.7224 - acc: 0.4592 - val_loss: 1.6298 - val_acc: 0.6040\n",
            "Epoch 10/25\n",
            "8083/8083 [==============================] - 3s 376us/step - loss: 1.7144 - acc: 0.4597 - val_loss: 1.6325 - val_acc: 0.6062\n",
            "Epoch 11/25\n",
            "8083/8083 [==============================] - 3s 375us/step - loss: 1.7138 - acc: 0.4496 - val_loss: 1.6554 - val_acc: 0.6151\n",
            "Epoch 12/25\n",
            "8083/8083 [==============================] - 3s 381us/step - loss: 1.6797 - acc: 0.4623 - val_loss: 1.7019 - val_acc: 0.6007\n",
            "Epoch 13/25\n",
            "8083/8083 [==============================] - 3s 378us/step - loss: 1.6798 - acc: 0.4578 - val_loss: 1.6927 - val_acc: 0.6040\n",
            "Epoch 14/25\n",
            "8083/8083 [==============================] - 3s 382us/step - loss: 1.6626 - acc: 0.4565 - val_loss: 1.7207 - val_acc: 0.5940\n",
            "Epoch 15/25\n",
            "8083/8083 [==============================] - 3s 383us/step - loss: 1.6602 - acc: 0.4634 - val_loss: 1.7245 - val_acc: 0.6151\n",
            "Epoch 16/25\n",
            "8083/8083 [==============================] - 3s 379us/step - loss: 1.6539 - acc: 0.4597 - val_loss: 1.7887 - val_acc: 0.6085\n",
            "Epoch 17/25\n",
            "8083/8083 [==============================] - 3s 379us/step - loss: 1.6233 - acc: 0.4698 - val_loss: 1.7600 - val_acc: 0.6107\n",
            "Epoch 18/25\n",
            "8083/8083 [==============================] - 3s 379us/step - loss: 1.6351 - acc: 0.4714 - val_loss: 1.7469 - val_acc: 0.6118\n",
            "Epoch 19/25\n",
            "8083/8083 [==============================] - 3s 378us/step - loss: 1.6270 - acc: 0.4774 - val_loss: 1.7843 - val_acc: 0.6096\n",
            "Epoch 20/25\n",
            "8083/8083 [==============================] - 3s 381us/step - loss: 1.6154 - acc: 0.4869 - val_loss: 1.8087 - val_acc: 0.6040\n",
            "Epoch 21/25\n",
            "8083/8083 [==============================] - 3s 381us/step - loss: 1.6406 - acc: 0.4856 - val_loss: 1.8517 - val_acc: 0.6140\n",
            "Epoch 22/25\n",
            "8083/8083 [==============================] - 3s 378us/step - loss: 1.6212 - acc: 0.4861 - val_loss: 1.8622 - val_acc: 0.6174\n",
            "Epoch 23/25\n",
            "8083/8083 [==============================] - 3s 378us/step - loss: 1.6271 - acc: 0.4830 - val_loss: 1.8621 - val_acc: 0.6263\n",
            "Epoch 24/25\n",
            "8083/8083 [==============================] - 3s 379us/step - loss: 1.6039 - acc: 0.4895 - val_loss: 1.8864 - val_acc: 0.6151\n",
            "Epoch 25/25\n",
            "8083/8083 [==============================] - 3s 381us/step - loss: 1.6239 - acc: 0.4881 - val_loss: 1.8473 - val_acc: 0.6185\n",
            "2246/2246 [==============================] - 0s 106us/step\n",
            "Test score: 1.7677100536978256\n",
            "Test accuracy: 0.6179875334457744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tjiGET1ynnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unV499vv1KBy",
        "colab_type": "text"
      },
      "source": [
        "# The importance of having sufficiently large intermediate layers\n",
        "\n",
        "      -The final outputs are 46-dimensional, you should avoid \n",
        "      intermediate layers with many fewer than 46 hidden units. \n",
        "\n",
        "      - Now we saw what happens when you introduce an information\n",
        "       bottleneck by having intermediate layers that are \n",
        "       significantly less than 46-dimensional: for example,\n",
        "        4 dimensional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKIYPPSC16_U",
        "colab_type": "text"
      },
      "source": [
        "The network now peaks at ~61% validation accuracy, 18% absolute drop. \n",
        "\n",
        "\n",
        "# Reason ?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This drop is mostly due to the fact that we’re trying to compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate\n",
        "space that is too low-dimensional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqU66Lii1JKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A5Ph_tC1JQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkPWzKzp9vRj",
        "colab_type": "text"
      },
      "source": [
        "# 3. Linear regression \n",
        "- Predict the median price of homes in a given Boston suburb in the mid-1970s, given data points about the suburb at the time, such as the crime rate, the local property tax rate, and so on.\n",
        "\t- Data: The Boston Housing Price dataset available from keras \n",
        "\t- Build a model \n",
        "\t- Train and validate \n",
        "\t- Make use of  k-fold cross validation \n",
        "\t- Plot the training and validation results \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSgY0tSL9twa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGBwL-pQ9t54",
        "colab_type": "code",
        "outputId": "61863945-a6ce-4f8c-d6df-b83462cd8684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LbLYRAL9t-5",
        "colab_type": "code",
        "outputId": "2f0d0dae-c8a5-4f93-eda4-33d2cee14f5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from keras.datasets import boston_housing\n",
        "\n",
        "(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 3us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjI-Tc_t9uH3",
        "colab_type": "code",
        "outputId": "4de66254-d6d9-45a4-efd7-8dd70b24df81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(404, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdh3boHp9ttL",
        "colab_type": "code",
        "outputId": "6e82944c-789c-4707-f793-3941aec502c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJmBzaWH9tri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JExuoHO-UUE",
        "colab_type": "text"
      },
      "source": [
        "As you can see, we have 404 training samples and 102 test samples. The data comprises 13 features. The 13 features in the input data are as follow:\n",
        "\n",
        "      -Per capita crime rate.\n",
        "      -Proportion of residential land zoned for lots over 25,000 \n",
        "      square feet.\n",
        "      -Proportion of non-retail business acres per town.\n",
        "      -Charles River dummy variable (= 1 if tract bounds river; \n",
        "      0 otherwise).\n",
        "      -Nitric oxides concentration (parts per 10 million).\n",
        "      -Average number of rooms per dwelling.\n",
        "      -Proportion of owner-occupied units built prior to 1940.\n",
        "      -Weighted distances to five Boston employment centres.\n",
        "      -Index of accessibility to radial highways.\n",
        "      -Full-value property-tax rate per $10,000.\n",
        "      -Pupil-teacher ratio by town.\n",
        "      -1000 * (Bk - 0.63) ** 2 where Bk is the proportion of \n",
        "          Black people by town.\n",
        "      -% lower status of the population.\n",
        "      \n",
        "The targets are the median values of owner-occupied homes, in thousands of dollars:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D84CKxlT9toR",
        "colab_type": "code",
        "outputId": "ee7f1136-c618-4759-d29f-12ed835b7444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        }
      },
      "source": [
        "train_targets"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4, 12.1,\n",
              "       17.9, 23.1, 19.9, 15.7,  8.8, 50. , 22.5, 24.1, 27.5, 10.9, 30.8,\n",
              "       32.9, 24. , 18.5, 13.3, 22.9, 34.7, 16.6, 17.5, 22.3, 16.1, 14.9,\n",
              "       23.1, 34.9, 25. , 13.9, 13.1, 20.4, 20. , 15.2, 24.7, 22.2, 16.7,\n",
              "       12.7, 15.6, 18.4, 21. , 30.1, 15.1, 18.7,  9.6, 31.5, 24.8, 19.1,\n",
              "       22. , 14.5, 11. , 32. , 29.4, 20.3, 24.4, 14.6, 19.5, 14.1, 14.3,\n",
              "       15.6, 10.5,  6.3, 19.3, 19.3, 13.4, 36.4, 17.8, 13.5, 16.5,  8.3,\n",
              "       14.3, 16. , 13.4, 28.6, 43.5, 20.2, 22. , 23. , 20.7, 12.5, 48.5,\n",
              "       14.6, 13.4, 23.7, 50. , 21.7, 39.8, 38.7, 22.2, 34.9, 22.5, 31.1,\n",
              "       28.7, 46. , 41.7, 21. , 26.6, 15. , 24.4, 13.3, 21.2, 11.7, 21.7,\n",
              "       19.4, 50. , 22.8, 19.7, 24.7, 36.2, 14.2, 18.9, 18.3, 20.6, 24.6,\n",
              "       18.2,  8.7, 44. , 10.4, 13.2, 21.2, 37. , 30.7, 22.9, 20. , 19.3,\n",
              "       31.7, 32. , 23.1, 18.8, 10.9, 50. , 19.6,  5. , 14.4, 19.8, 13.8,\n",
              "       19.6, 23.9, 24.5, 25. , 19.9, 17.2, 24.6, 13.5, 26.6, 21.4, 11.9,\n",
              "       22.6, 19.6,  8.5, 23.7, 23.1, 22.4, 20.5, 23.6, 18.4, 35.2, 23.1,\n",
              "       27.9, 20.6, 23.7, 28. , 13.6, 27.1, 23.6, 20.6, 18.2, 21.7, 17.1,\n",
              "        8.4, 25.3, 13.8, 22.2, 18.4, 20.7, 31.6, 30.5, 20.3,  8.8, 19.2,\n",
              "       19.4, 23.1, 23. , 14.8, 48.8, 22.6, 33.4, 21.1, 13.6, 32.2, 13.1,\n",
              "       23.4, 18.9, 23.9, 11.8, 23.3, 22.8, 19.6, 16.7, 13.4, 22.2, 20.4,\n",
              "       21.8, 26.4, 14.9, 24.1, 23.8, 12.3, 29.1, 21. , 19.5, 23.3, 23.8,\n",
              "       17.8, 11.5, 21.7, 19.9, 25. , 33.4, 28.5, 21.4, 24.3, 27.5, 33.1,\n",
              "       16.2, 23.3, 48.3, 22.9, 22.8, 13.1, 12.7, 22.6, 15. , 15.3, 10.5,\n",
              "       24. , 18.5, 21.7, 19.5, 33.2, 23.2,  5. , 19.1, 12.7, 22.3, 10.2,\n",
              "       13.9, 16.3, 17. , 20.1, 29.9, 17.2, 37.3, 45.4, 17.8, 23.2, 29. ,\n",
              "       22. , 18. , 17.4, 34.6, 20.1, 25. , 15.6, 24.8, 28.2, 21.2, 21.4,\n",
              "       23.8, 31. , 26.2, 17.4, 37.9, 17.5, 20. ,  8.3, 23.9,  8.4, 13.8,\n",
              "        7.2, 11.7, 17.1, 21.6, 50. , 16.1, 20.4, 20.6, 21.4, 20.6, 36.5,\n",
              "        8.5, 24.8, 10.8, 21.9, 17.3, 18.9, 36.2, 14.9, 18.2, 33.3, 21.8,\n",
              "       19.7, 31.6, 24.8, 19.4, 22.8,  7.5, 44.8, 16.8, 18.7, 50. , 50. ,\n",
              "       19.5, 20.1, 50. , 17.2, 20.8, 19.3, 41.3, 20.4, 20.5, 13.8, 16.5,\n",
              "       23.9, 20.6, 31.5, 23.3, 16.8, 14. , 33.8, 36.1, 12.8, 18.3, 18.7,\n",
              "       19.1, 29. , 30.1, 50. , 50. , 22. , 11.9, 37.6, 50. , 22.7, 20.8,\n",
              "       23.5, 27.9, 50. , 19.3, 23.9, 22.6, 15.2, 21.7, 19.2, 43.8, 20.3,\n",
              "       33.2, 19.9, 22.5, 32.7, 22. , 17.1, 19. , 15. , 16.1, 25.1, 23.7,\n",
              "       28.7, 37.2, 22.6, 16.4, 25. , 29.8, 22.1, 17.4, 18.1, 30.3, 17.5,\n",
              "       24.7, 12.6, 26.5, 28.7, 13.3, 10.4, 24.4, 23. , 20. , 17.8,  7. ,\n",
              "       11.8, 24.4, 13.8, 19.4, 25.2, 19.4, 19.4, 29.1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgxJyUdL9tjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRlupGCi-3RV",
        "colab_type": "text"
      },
      "source": [
        "The prices are typically between \\$10,000 and \\$50,000. If that sounds cheap, remember this was the mid-1970s, and these prices are not inflation-adjusted.\n",
        "\n",
        "# Preparing the data\n",
        "\n",
        "It would be problematic to feed into a neural network values that all take wildly different ranges. \n",
        "\n",
        "The network might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult.\n",
        "\n",
        " A widespread best practice to deal with such data is to do feature-wise normalization: for each feature in the input data (a column in the input data matrix), we will subtract the mean of the feature and divide by the standard deviation, so that the feature will be centered around 0 and will have a unit standard deviation. This is easily done in Numpy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pIk8R0K-8tz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data -= mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data /= std\n",
        "\n",
        "test_data -= mean\n",
        "test_data /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB1e4Etf-_9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ysBsmkO_GRD",
        "colab_type": "text"
      },
      "source": [
        "**Note** that the quantities that we use for normalizing the test data have been computed using the training data. We should never use in our workflow any quantity computed on the test data, even for something as simple as data normalization.\n",
        "\n",
        "# Building our network\n",
        "\n",
        "Because so few samples are available, we will be using a very small network with two hidden layers, each with 64 units. \n",
        "\n",
        "In general, the less training data you have, the worse overfitting will be, and using a small network is one way to mitigate overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfE3nhYR_LS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "def build_model():\n",
        "    # Because we will need to instantiate\n",
        "    # the same model multiple times,\n",
        "    # we use a function to construct it.\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(64, activation='relu',\n",
        "                           input_shape=(train_data.shape[1],)))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(1))\n",
        "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvvvB_hI_OyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra2YxwRn_UtZ",
        "colab_type": "text"
      },
      "source": [
        "Our network ends with a single unit, and no activation (i.e. it will be linear layer). This is a typical setup for scalar regression (i.e. regression where we are trying to predict a single continuous value). Applying an activation function would constrain the range that the output can take; for instance if we applied a sigmoid activation function to our last layer, the network could only learn to predict values between 0 and 1. Here, because the last layer is purely linear, the network is free to learn to predict values in any range.\n",
        "\n",
        "Note that we are compiling the network with the mse loss function -- Mean Squared Error, the square of the difference between the predictions and the targets, a widely used loss function for regression problems.\n",
        "\n",
        "We are also monitoring a new metric during training: mae. This stands for Mean Absolute Error. It is simply the absolute value of the difference between the predictions and the targets. For instance, a MAE of 0.5 on this problem would mean that our predictions are off by \\$500 on average.\n",
        "\n",
        "# Validating our approach using K-fold validation\n",
        "\n",
        "To evaluate our network while we keep adjusting its parameters (such as the number of epochs used for training), we could simply split the data into a training set and a validation set, as we were doing in our previous examples. However, because we have so few data points, the validation set would end up being very small (e.g. about 100 examples). A consequence is that our validation scores may change a lot depending on which data points we choose to use for validation and which we choose for training, i.e. the validation scores may have a high variance with regard to the validation split. This would prevent us from reliably evaluating our model.\n",
        "\n",
        "The best practice in such situations is to use K-fold cross-validation. It consists of splitting the available data into K partitions (typically K=4 or 5), then instantiating K identical models, and training each one on K-1 partitions while evaluating on the remaining partition. The validation score for the model used would then be the average of the K validation scores obtained.\n",
        "\n",
        "In terms of code, this is straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPHRCSZh_X1H",
        "colab_type": "code",
        "outputId": "73813d23-0bee-4735-bc38-ba4b6a8c1f86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "k = 4\n",
        "num_val_samples = len(train_data) // k\n",
        "num_epochs = 100\n",
        "all_scores = []\n",
        "\n",
        "for i in range(k):\n",
        "    print('processing fold #', i)\n",
        "    # Prepare the validation data: data from partition # k\n",
        "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "\n",
        "    # Prepare the training data: data from all other partitions\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[:i * num_val_samples],\n",
        "         train_data[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    partial_train_targets = np.concatenate(\n",
        "        [train_targets[:i * num_val_samples],\n",
        "         train_targets[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "\n",
        "    # Build the Keras model (already compiled)\n",
        "    model = build_model()\n",
        "    # Train the model (in silent mode, verbose=0)\n",
        "    model.fit(partial_train_data, partial_train_targets,\n",
        "              epochs=num_epochs, batch_size=1, verbose=0)\n",
        "    # Evaluate the model on the validation data\n",
        "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
        "    all_scores.append(val_mae)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 0\n",
            "processing fold # 1\n",
            "processing fold # 2\n",
            "processing fold # 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqTLcPV7_nMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PUb03T9B4P8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiR6D0Z8B4L_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gn1pJBe_r0d",
        "colab_type": "code",
        "outputId": "5e8292b1-e3f3-4b9f-98dd-48d7bbd73efe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "all_scores"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.1080476930826015, 2.3698492970797096, 2.729290190309581, 2.472803566715505]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5apXjwQK_sH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfvyTLDR_sRD",
        "colab_type": "code",
        "outputId": "bee1c64b-0777-4e7e-ce71-90c4b7555023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "np.mean(all_scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.419997686796849"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnUoQ2GhANMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-T7fFiLB6r7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PowOCc1LB7ky",
        "colab_type": "text"
      },
      "source": [
        "# Let's try training the network for a bit longer: 150 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNgJwDQnB6ny",
        "colab_type": "code",
        "outputId": "efaeecaf-672c-4709-a2d3-332ad54c2a1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "# Some memory clean-up\n",
        "K.clear_session()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu4iThczB6lu",
        "colab_type": "code",
        "outputId": "44c85e08-ae75-469c-86c2-8164931dc0f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "num_epochs = 150\n",
        "all_mae_histories = []\n",
        "for i in range(k):\n",
        "    print('processing fold #', i)\n",
        "    # Prepare the validation data: data from partition # k\n",
        "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "\n",
        "    # Prepare the training data: data from all other partitions\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[:i * num_val_samples],\n",
        "         train_data[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    partial_train_targets = np.concatenate(\n",
        "        [train_targets[:i * num_val_samples],\n",
        "         train_targets[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "\n",
        "    # Build the Keras model (already compiled)\n",
        "    model = build_model()\n",
        "    # Train the model (in silent mode, verbose=0)\n",
        "    history = model.fit(partial_train_data, partial_train_targets,\n",
        "                        validation_data=(val_data, val_targets),\n",
        "                        epochs=num_epochs, batch_size=1, verbose=0)\n",
        "    mae_history = history.history['val_mean_absolute_error']\n",
        "    all_mae_histories.append(mae_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 0\n",
            "processing fold # 1\n",
            "processing fold # 2\n",
            "processing fold # 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1IdOL0wB6gF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cnkkpaXANax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "071kN3RqATN8",
        "colab_type": "text"
      },
      "source": [
        "We can then compute the average of the per-epoch MAE scores for all folds:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DILjETC7ANld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thpck5iL_sYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_mae_history = [\n",
        "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl4NglnoAY9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8LB_yRqAc6F",
        "colab_type": "text"
      },
      "source": [
        "Let's plot this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf3PgUk8AZDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD-3HDIiAY6g",
        "colab_type": "code",
        "outputId": "692727de-05ec-4064-ef77-a1e4f4cdbb4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation MAE')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3xc1Znw8d8zRb1Xy5Js2Za7cccF\nHMCmxARiJwtkCYSEwIYkEMImvEsgeTfJsnmzSzolG0JIcRYIhBZKgGCMaQZsy8a9yrZsWZat3rvm\nvH/cO6MZaSRLtkcjM8/389HHM3Pv3Dm61tznnnOec44YY1BKKRW5HOEugFJKqfDSQKCUUhFOA4FS\nSkU4DQRKKRXhNBAopVSEc4W7AEOVkZFhCgoKwl0MpZQ6q2zatKnKGJMZbNtZFwgKCgooKioKdzGU\nUuqsIiKH+9umTUNKKRXhQh4IRMQpIh+JyMtBtt0oIpUissX++ZdQl0cppVSg4WgaugPYDST1s/0p\nY8w3hqEcSimlgghpjUBE8oArgEdD+TlKKaVOXaibhn4F3AV4BtjnKhHZJiLPiEh+sB1E5BYRKRKR\nosrKypAUVCmlIlXIAoGIXAlUGGM2DbDbS0CBMWYmsBpYFWwnY8wjxpj5xpj5mZlBs5+UUkqdolDW\nCM4HVohICfAksExEHvPfwRhTbYxpt58+CswLYXmUUkoFEbJAYIy5xxiTZ4wpAK4F3jTGfMF/HxHJ\n8Xu6AqtTOST2Hm/k56/vpbqp/eQ7K6VUBBn2cQQicq+IrLCfflNEdorIVuCbwI2h+tziiiYefLOY\nqqaOUH2EUkqdlYZlZLEx5i3gLfvx9/1evwe4ZzjK4HIKAJ3dA/VbK6VU5ImYkcVuOxB0eXRFNqWU\n8hcxgcDlsH7VLq0RKKVUgMgJBL6mIa0RKKWUv4gJBG6nXSPwaI1AKaX8RUwgcDnsPgKtESilVICI\nCQTeGoFmDSmlVKCICQQuzRpSSqmgIicQOLRGoJRSwURMIPCNI9A+AqWUChAxgcClWUNKKRVUxAQC\nt0PHESilVDAREwh8NQLtI1BKqQARFAg0a0gppYKJmEDg9mUNaSBQSil/ERMIfDUCbRpSSqkAkRMI\nvJ3F2jSklFIBIiYQiAguh2iNQCmleomYQABW85B2FiulVKCICgRuh0OnmFBKqV4iKhC4nKJTTCil\nVC8RFggcOsWEUkr1ElGBwO0QHUeglFK9RFQgcDkdmjWklFK9RFggEB1HoJRSvURUIHA7tEaglFK9\nRVQgcDm1j0AppXqLsECg4wiUUqq3iAoEboeOI1BKqd4iKhBYU0xojUAppfxFVCBwOx3aR6CUUr1E\nVCBwObRGoJRSvUVWIHA6tI9AKaV6iahA4HaKZg0ppVQvERUIXA6HrkeglFK9RFYg0GmolVKqj4gK\nBLowjVJK9RXyQCAiThH5SEReDrItWkSeEpFiEVkvIgWhLIsuVamUUn0NR43gDmB3P9tuBmqNMYXA\nL4H7QlkQt04xoZRSfYQ0EIhIHnAF8Gg/u6wEVtmPnwEuFhEJVXnc2keglFJ9hLpG8CvgLqC/2/Bc\noBTAGNMF1APpoSqMLlWplFJ9hSwQiMiVQIUxZtMZONYtIlIkIkWVlZWnfBzvUpXGaK1AKaW8Qlkj\nOB9YISIlwJPAMhF5rNc+ZUA+gIi4gGSguveBjDGPGGPmG2PmZ2ZmnnKBXE7r1+3WDmOllPIJWSAw\nxtxjjMkzxhQA1wJvGmO+0Gu3F4Ev2Y+vtvcJ2VXa5bS6HzRzSCmlegz7OAIRuVdEVthPfw+ki0gx\n8G3g7lB+ttth/bqaOaSUUj1cw/Ehxpi3gLfsx9/3e70NuGY4ygB+NQLNHFJKKZ+IGlns7SPo1Mwh\npZTyiahA4HZojUAppXqLqEDgrRFoIFBKqR4RFQjcdh+BNg0ppVSPiAoELofWCJRSqrfICgTeGoGm\njyqllE9EBQK3DihTSqk+IioQ9DQNaY1AKaW8IisQ+JqGtEaglFJeERUI3N70Uc0aUkopn4gKBC4d\nUKaUUn1EVCDw1gg0a0gppXpEVCDQaaiVUqqvyAoEOg21Ukr1EVGBwK3TUCulVB8RFQhcmjWklFJ9\nRFQg8E5DreMIlFKqR7+BQETu8nt8Ta9tPw5loUKlZxpqrREopZTXQDWCa/0e39Nr2/IQlCXkNGtI\nKaX6GigQSD+Pgz0/K/QsXq+BQCmlvAYKBKafx8GenxV6Fq/XpiGllPJyDbBtlog0YN39x9qPsZ/H\nhLxkIeCdYqJTm4aUUsqn30BgjHEOZ0GGg4jgcojWCJRSys+Q0kdFJF5EviAifw9VgULN5RTtLFZK\nKT8nDQQiEiUinxWRp4Fy4GLg4ZCXLETcDodOMaGUUn76bRoSkcuAzwOXAWuBPwPnGmO+PExlCwmX\nU3SKCaWU8jNQjeA1YDywxBjzBWPMS8BZfyvtcjp0igmllPIzUNbQXKxBZW+IyEHgSeCs70B2O0TH\nESillJ9+awTGmC3GmLuNMROAHwCzAbeIvCoitwxbCc8wl9OhWUNKKeVnUFlDxpj3jTG3A3nAL4FF\nIS1VCLmcouMIlFLKz0CdxXP72VQFPBSa4oSe26E1AqWU8jdQH0ERsAPrwg+B8wsZYFmoChVKmjWk\nlFKBBgoE3wauBlqxOoqfN8Y0DUupQsjldGjTkFJK+Rmos/hXxpglwO1APrBGRP4qIrOHrXQh4NYp\nJpRSKsBJO4uNMQeBF4DXgQXApFAXKpS0aUgppQIN1Fk8HmscwUqgFKt56MfGmNZhKltIuJ0Omrq6\nwl0MpZQaMQbqIygGtmHVBhqAMcDXRaw+Y2PML0JeuhCwZh/VGoFSSnkNFAjupWcBmoShHlhEYoB3\ngGj7c54xxvyg1z43Aj8FyuyXHjLGPDrUzxoKl1MnnVNKKX8DrUfww9M8djuwzBjTJCJu4D0RedUY\n82Gv/Z4yxnzjND9r0Nw6DbVSSgUYqEZwWowxBvCmm7rtn7BfgV06oEwppQIMaWGaoRIRp4hsASqA\n1caY9UF2u0pEtonIMyKSH8rygD3FhPYRKKWUT0gDgTGm2xgzG2uOogUiMqPXLi8BBcaYmcBqYFWw\n44jILSJSJCJFlZWVp1Umt0OnoVZKKX8nbRoSkWjgKqDAf39jzL2D/RBjTJ2IrAWWY01b4X292m+3\nR4Gf9PP+R4BHAObPn39at/M6jkAppQINpkbwAtZYgi6g2e9nQCKSKSIp9uNY4FJgT699cvyergB2\nD67Yp86tWUNKKRVgMJ3FecaY5adw7BxglYg4sQLOX40xL4vIvUCRMeZF4JsisgIryNQAN57C5wyJ\ny6FZQ0op5W8wgeB9ETnHGLN9KAc2xmwD5gR5/ft+j+8B7hnKcU+XtTCNBgKllPIaTCBYAtwoIoew\nxgYIVnbozJCWLETcTqGj24MxBu8oaaWUimSDCQSXh7wUw8jlsLpFuj0Gl1MDgVJKDWb20cNACvBp\n+yfFfu2s5L34az+BUkpZThoIROQO4HEgy/55TERuD3XBQsVtBwLNHFJKKctgmoZuBhYaY5oBROQ+\n4APgwVAWLFS8TUPaYayUUpbBjCMQoNvveTeB6xefVXw1Ah1drJRSwOBqBH8E1ovI8/bzzwC/D12R\nQsvl1BqBUkr5O2kgMMb8QkTewkojBfiyMeajkJYqhFwOu7NYA4FSSgEDL1WZZIxpEJE0oMT+8W5L\nM8bUhL54Z57brhFo05BSSlkGqhE8AVwJbCJwHQGxn48PYblCxpc+qjUCpZQCBl6h7Er733HDV5zQ\n82YNafqoUkpZBjOOYM1gXjtbRLl0HIFSSvkbqI8gBogDMkQklZ6U0SQgdxjKFhIJ0W4Amtu7T7Kn\nUkpFhoH6CL4K/CswGqufwBsIGoCHQlyukEmKtX7lhrbOMJdEKaVGhoH6CO4H7heR240xZ+Uo4mCS\nYqwaQUOrBgKllILBjSN40F5reBoQ4/f6n0NZsFBJirUDgdYIlFIKGNyaxT8ALsIKBK9gTUv9HnBW\nBoL4KCcOgYbWrnAXRSmlRoTBzDV0NXAxcNwY82VgFpAc0lKFkIiQFOvWGoFSStkGEwhajTEeoEtE\nkoAKID+0xQqtxBgXjW1aI1BKKRjcpHNFIpIC/A4re6gJaxrqs1ZSjFs7i5VSyjaYzuJb7YcPi8hr\nQJK9MP1ZKylGm4aUUsproAFlcwfaZozZHJoihV5SrIuSqpZwF0MppUaEgWoEP7f/jQHmA1uxBpXN\nBIqAxaEtWuhojUAppXr021lsjFlqjFkKlANzjTHzjTHzgDlA2XAVMBSSYt3aWayUUrbBZA1NNsZs\n9z4xxuwApoauSKGXGOOiqb2LLp14TimlBpU1tE1EHgUes59fD5z1ncUATe1dpMRFhbk0SikVXoOp\nEXwZ2AncYf/ssl87a/mmmdDRxUopNaj00Tbgl/bPx0JSjM5AqpRSXgOlj/7VGPM5EdlO4FKVABhj\nZoa0ZCHUUyPQQKCUUgPVCO6w/71yOAoynHxTUWvmkFJKDbgeQbn97+HhK87wSNSmIaWU8hmoaaiR\nIE1CWIPKjDEmKWSlCjFtGlJKqR4D1QgSh7Mgwykx2oWINg0ppRQMbhwBACKSReAKZUdCUqJh4HAI\nCdEurREopRSDGEcgIitEZD9wCHgbKAFeDXG5Qk7nG1JKKctgBpT9J7AI2GeMGYe1WtmHIS3VMND5\nhpRSyjKYQNBpjKkGHCLiMMasxZqN9KyWFKNNQ0opBYMLBHUikgC8AzwuIvcDzSd7k4jEiMgGEdkq\nIjtF5D+C7BMtIk+JSLGIrBeRgqH+AqcqMcatncVKKcXgAsFKoBX4FvAacAD49CDe1w4sM8bMAmYD\ny0VkUa99bgZqjTGFWFNY3DfYgp+upFitESilFAwQCETk1yJyvjGm2RjTbYzpMsasMsY8YDcVDchY\nmuynbvun97iElcAq+/EzwMUiIqfwewyZdhYrpZRloBrBPuBnIlIiIj8RkTlDPbiIOEVkC1ABrDbG\nrO+1Sy5QCmCM6QLqgfQgx7lFRIpEpKiysnKoxQgqKdZNU3sXHk+wMXNKKRU5Blqh7H5jzGLgQqAa\n+IOI7BGRH4jIpMEc3K5JzAbygAUiMuNUCmmMecReIW1+ZmbmqRyij6QYF8ZAU4f2EyilIttJ+wiM\nMYeNMfcZY+YAnwc+A+weyocYY+qAtcDyXpvKgHwAEXEByVhBJ+R0mgmllLIMZkCZS0Q+LSKPYw0k\n2wv80yDelykiKfbjWOBSYE+v3V4EvmQ/vhp40xgzLG01vjUJdHEapVSEG2jSuUuxagCfAjYATwK3\nGGNOmjpqywFWiYgTK+D81RjzsojcCxQZY14Efg/8r4gUAzXAtaf+qwyNt0ZQ19oxXB+plFIj0kBz\nDd0DPAHcaYypHeqBjTHbgD4dzMaY7/s9bgOuGeqxz4TRybEAHKtrC8fHK6XUiDHQ7KPLhrMgw210\nSiwicKSmJdxFUUqpsBrMgLKPpSiXg5ykGI5qIFBKRbiIDQQAeWlxlNZqIFBKRbaIDgT5qXGU1rSG\nuxhKKRVWER0IxqTFcbyhjbbO7nAXRSmlwiaiA0F+mpU5VFantQKlVOSK8EAQB0CpdhgrpSJYZAeC\nVDsQ1GqNQCkVuSI6EGQlRhPlcmiNQCkV0SI6EDgcQl5KrAYCpVREi+hAAFY/gY4lUEpFMg0EabE6\nlkApdUpe2nqM6x/9kGGaNDlkNBCkxlHf2km9rkuglBqijSU1rCuupqKxPdxFOS0aCDSFVCl1impb\nrBvI4oqmk+w5skV8IJiQmQDA9rL6MJdEKXW2qWux1jPZf6IxzCU5PREfCCZlJzAuI54XtxwLd1GU\nUiPc37eV8+NXelbq9TYp79cawdlNRFg5ezQfHqrmeL0uUqOU6t/ftx/jL+uP+J7XadPQx8fK2bkY\nAy9uLQt3UZRSI1hVYweN7V20d1kTVdbaTUMaCD4GxmXEMysvmRe0eUgpNYCqJis7qLa5k65uD41t\nXSTGuKhu7qCm+exd/1wDgW3F7Fx2HmuguOLs7vRRSoVOpR0IqpraaWjrAmDe2FTg7K4VaCCwXTo1\nG4ANh2rDXBKl1EjU1tlNo33xr2nu8GUMnVuQBmgg+FjITY3F7RQO1zT7XttSWkd9iw40U0pBtV/T\nT01zh28MwdScROKinOw/i1sTNBDYnA4hPy2OI9XWwLKKhjb+6X/Wcdmv3ubtfZVhLp1SKtyq/EYP\nVzW1U99qBYbUuCgmZCZojeDjYmxaHIftQLD3RCMeA13dhi/9YQOv7Tge5tIpBU9tPMLdz24LdzEi\nUqVfILCahqwaQUpcFBOzNBB8bIxNj+dwdTPGGPafsP5TX7x9CQnRLj44UBXm0ikFb+2t5KWtmt0W\nDt6MIYf0CgSxbqaNTqK8vu2sHWGsgcDP2PQ4mju6qW7uYH9FE6lxbkYnxzA2PY7DOheRGgFqWzpo\n7uimsU37roabNxAUpMdT1WR1FotAUqybq+bmEet28tt3Doa5lKdGA4GfsenWBHSHq1sormikMCsB\nEbECQfWZCwSNbZ0cq9Opr9XQ1TZbAeBEw9k922UodHV7+Nk/9rLhUE1Ijl/V1EFijIuclBhqmtup\na+0kKcaN0yGkxkfxz+fm88KWMsrrQ/Pd/o+XdvLu/tD0V2og8DM2PR6Aw9XN7K9oojAr0ff60doW\nuro9Qzre5iO1Qf8of/76Pq55+IPTL7CKODV2ymJFg06H0tuu8gYeWlvMPz/yAfe+tMs3+vdMqWxq\nJzMhmrT4aF/TUEqc27f95iXj8Bj4w3uH+j1Gt8ew61jDkD/7nX2V/HFdySm9dzA0EPjJS41FxLqA\n17V0MjHLmpm0ID2Ozm5D+RDnIvruc9u59fHNdPYKIEdrWymraz2rRyKq4WeModb+mznRqIGgN29n\n7fLpo/jDukM8uaHUt+23bx9gd/npXUSrGtvJSIgmPT6K6qYO6lo7SYntCQT5aXFcOTOHx9cfYV+Q\nvoLa5g5u/OMGPvXAu7y3f/B9jl3dHn70912MSYvjxvMLTut36I8GAj/RLiejk2NZs7sCgInZViAY\nk2bVFEqqmwP233CohqP9LHPZ2NbJ3hONVDW1s3ZPRcA27/wkwf5YlOpPU3sXXR5rJSxtGuqruKIJ\nl0N44PNzyEmOYfMRa3BoRUMb//XqHn65et9Jj+HxGLo9wVcbq2pqJyMxivT4KBrbu6hoaCMlLipg\nn/9z2WQSol184dH1HPa7XpTVtfLph95j/cEaolwO/rFz8FmIf9lYyr4TTXz3U1OJdjkH/b6h0EDQ\ny9j0ON+df6G3RpDR03fgZYzhK38u4o4ntwRdpm5LaR3GWBkGT20sDdimgUCdCm//AMCJs7hp6GBl\nE2v3Vpx8xyEqrmhibHocbqeDGbnJvjVGPiqtA6yMq/5WIjTG8PK2Yyz+7zXc+dctQfepauogIyGa\ntATr4n+oqjmgaQisWsFj/7KQzm4P1/1uva8Gd9+re6hqauepry7iokmZrNl9YlDLW3Z1e/jV6n0s\nGp/GJ6dnD+5EnAINBL14+wkSol2MSooBIDsxhiiXIyDC17ZYy1tuOlzLhwf79gNsOlyLCFy/cCxr\n91YEfHG9fxwaCNRQePsHACr6qRGU1bXyh/cOhWUN3X9ZVcS/Pb31pPv9+JU9fPXPm2jp6BrS8Y0x\nbLUv6sEUVzb5FpqamZvMwcpmGts62WK/p6Pbw+v93Inf+fRWvvHER9Q2d7Jmd0WfWkF7Vzf1rZ1k\nJkSTHh9tv+YJaBrympSdyJ++vICKxja++/x2dpTV8+LWY9y8ZBxzxqRyydRsjtW3sWsQTVUHKpup\nbu7g2nPHICIn3f9UaSDoxZs55M0YAnA4JGCwGRAQFB5au7/PcTYdrmVydiI32R1Iz2w6ClidRXX2\nXcm+E2fvABQ1/Lw1yVi3s98awVMbS7n35V3DPgCypaOLt/ZW8OzmowMu+9rR5eGDA1V0dHtYH+QG\naiCv7jjOyl+vY9vRvsGgs9vDkeoWXy1+Rl4yADuPNbDlSB0zcpPIT4vlxSBjMDYcquG5zWV85RPj\n+O+rzqGxvatPp2x1k3XuMxKjSU/oaQ5K7tU05DUrP4VvXzqZV3cc519WFZES5+arF04AYOmULETw\nNUEDNLR18tn/Wccbu04EHMdbq5mRmzzwyTlNGgh6KfALBP56p5Aesf/YPzc/j3XF1b72SLAu9luO\n1DG/IJVxGfGcW5Dq+2I2tHZijDWlxb4TjWG5c1NnJ29NctKoxH47iw9VWTcoP31975Cz3E7m6aJS\nDlYGv3nZfLiOLo/BY+BP75f0e4zNR2pp7rCyeYY6dYv3wum9SNc0d3DD7622+MPVzXR5jO97e459\n4dxaWse2o3XMyU/l0zNH8/6Bat94ALBqGT97fS+ZidF8+9LJnDchA4D1h6oDPtv7noyEaNLiey7+\nqXF9awRet1wwngUFaRxvaOMbSwtJirH2zUyMZlZeCmt291z01x+s4aMjddz2xGY2lvQEyB1l9cRF\nORmXET+kczVUGgh68XYMT+wTCOI5XNPsu3B7g8Ldl08lNc4d0BG170Qjje1dvulpC7MSfbnF3ur9\nObnJ1LV0+qa1VepkvFlmU0clcqKhPehNxKGqJpJj3RysbObZzUf7bN9YUsO/Pb2137by/tQ2d/Bv\nz2zj9/2kRm44VI1D4JKpWTy1sbTfAW/v7KvE6RDmjU3lnZMEgrqWDv73w8N0ewwej/EFDu+ykB8c\nqObd/VU8sf6IL2PIGwgyEqIZnRzD8x+V0dzRzez8FFbMHk23x/DK9nLfZ7xXXMWGQzV8Y2khsVFO\nRtkDSNf3SvvuCQRRZNhNQ0CfPgJ/Tofw4HVzuOfyKdyweGzAtkunZbP1aL2vZrf5SC0uh5CbEsvN\nf9roC7g7yuqZPjoJpyN0zUKggaCPyaMSufWiCayYPTrg9bHpcbR1eqiw5xs5XN3CqKQY0uKjuG1p\nIe/ur/L9YW86bNUO5o2xpqfNToqmurmDzm6P765u4Xhr235tHlKDVNvSgdMhFGYl0NHl6XMxN8Zw\nqLKZz87JZXZ+Cr96Yz8dXVatoNtjuO+1PXzutx/w9Kaj/baV98db4+2vX2v9oRqmj07mmxdPpKm9\nq0+ChNc7+yuZOyaFK87J4WBV84DNSE8XHeXf/7aD13ceZ1d5g+9i7L3oe9NBX95W7vseefsIwGpO\n2XPcKu/sMSlMGZXElFGJ/LXIKpsxhp+/vo/clFiuXZDve9+CgjQ2ltTg8esnqGq0m4YSokmKdeGy\nL8wpscGbhryyk2L46oUT+mT7XDw1C4A37YzCzYdrmT46iVU3LaC9y8Oq90vo9hh2Hmtg+ujQNguB\nBoI+nA7hruVTyEmODXjd24lcYle9j9Q0MybNaka6YfFY8tNi+fEru2nv6ubtfZVkJESTn2YdIysx\nBmOsuwrvXd2icekA7D2uHcZqcGpbOkmNc5NtJzH0TiGtbGynuaOb8ZnxfO3CCZTXt/luSl7aeozf\nvHWAa+blkZEQxbrik+exbyyp8XXoeo+z93jf5sz2rm4+Kq1jwbg0ZualsGBcGo+8c9D33jW7T/CH\n9w5R0dDGjrIGLpiYyQWTMoGBm4e22e3jD79z0LffeRPS+wSCsrpWnt18lNHJMcRHu3zvn2n3EyTG\nuBhnf3+vWziGHWUNbD9az7riaraU1nHr0sAL9cLx6dS1WOnfvnNrB6HMxGhExNc8lDxAjWAgk7MT\nyU2JZc3uE3R2e9h6tI45Y1LJT4tj2ZQs/r69nP0VjbR2dvuauUIpZIFARPJFZK2I7BKRnSJyR5B9\nLhKRehHZYv98P1TlOV1j7Yu+d86hIzUtjLH7E6JdTu765BT2HG9k8X+9yepdJ7jinFG+zubsJKsq\nWdHQ7puoqjArgbT4qLDMYe7xWKmvzwVpOlAjV21zB6lxUX6BILCf4KB9kzIuI57zCtNxCLxvT5a4\ndm8FGQlR/Pc/zWTxhAzWHagesH+qqKSGax7+gIfeLAZ6AkFDW1efALTtaD0dXR4WjrNqud9ZPpmK\nxnZ+984hiiuauO2Jzdz78i5W/nodABdMymRCZjy5KbEDNg/tKKsn1u1ka2kdf1xXwrScJM6bkE5Z\nXSvN7V3sKm/gkqlZRLkclFS3MKFXc663g3V2fgoO+w5+5excYtwOnthwmF+vLSYrMZqr5+UFvM/7\ne7yx6wSPrz/Mr97Yx4ZDNSREu4hxWwHDGwhS++ksPhkR4ZKpWbxXXMWW0jraOj3MtZuSPz1rNFVN\nHTz6rtUMd07eWRwIgC7gTmPMNGARcJuITAuy37vGmNn2z70hLM9pyU2NJcrpYE95I22d3ZxoaPcF\nB4ArZ+Zw4aRM8tPiePSL8/nhium+bVmJPV9cbx9BWrw1dW2wzKH61s6AjqTT1dXt4fL73+V/PzwM\nWNkXq3edYM2eM5/LrUKnprmD1Pgo341F70BwyC8QJMW4mZmXwrriKjwew7v7q/jExEwcDmFJYTqV\nje39TptsjOG/Xt0DwAtbjtHRZd2xTstJAgi4UwZ806h4V+qaNzaNT50zit++c4BvPLGZWLeT7185\njbqWTtLjo5iRm4yIcNHkTN7eV+kLMv4a2jo5VNXMzUvGkRYfRVVTOxdNzvRN+7LpcC3l9W2cW5DG\nsslWM0vvBI+ZeSk4BOaOSfW9lhzr5tMzR/PMpqN8cLCaWy4Y36fZJj8tjtyUWH6+eh/fe34Hv3pj\nP2/vqyQ3paeVwJs5FCx9dLAumZZNW6eHB+1g6+1TXDo5i/goJ89uPkqM28H4EHcUQwgDgTGm3Biz\n2X7cCOwGckP1eaHmdjpYOD6Nt/ZW+DKGvDUCsCL8qpsW8MJt53PJtOyAnN8sb42gsZ3a5g6iXA7i\nopxMGZXI7vIG2joD50T54Ys7uXlV0RmbvGrnsQZ2lzfw47/v5kh1Cw++aaW7Hq3t//hd3Z6ANtLB\nMqb/kZnq9NS2dJAa5/bdWFQ0Bt6ZH6pqJsrlYLTdrLmkMIOtR+v58FA1Nc0dXDDJyojxZsb01zz0\n+q4TbDpc67v7fnz9Ydo6PXx+4RgA9h4PTK1cf6iGydmJpPpl09z1ySl0dnvYc7yR/75qJjctGcfr\n37qAJ76yyNfxecclExmVHHJ6cP0AAB0xSURBVMNNf9rInl7H3GE3C507Lo0v2h2tF07K9I32907F\nPTUnydef1zsQpMVH8devLuYrF4wPeP3zC8fQ2W1IiXPz+QVjgp6Db106iRvPK+Clbyxh973LeeG2\n83n0S/N9271jCZJOIxAsHJdOQrSLd/ZVkp1kdW4DxEY5uXRaNsbAtJwkXM7Qt+APSx+BiBQAc4D1\nQTYvFpGtIvKqiEwPsh0RuUVEikSkqLIyfKuFXTI1m4NVzbxlj4r09hucTHp8FA6xhrp7v8wiwqXT\nRtHS0e3rMAKr3fNvW8p8jwfDGMPaPRV95jTy8k9Hu+7RD9lzvJGMhGjKBggEN68q4vYnPzrpZx+s\nbOJvH5X5nj+3uYxz/98bfYKbOn21LZ2kxUcRG+UkKcYVtEZQkB7nawY5rzCdbo/hZ//YC8AnJlrt\n8vlpcYxJi+O94sAUSbBuAH7y2h4mZMbzm+vnEe1y+DLiLpmaRVZiNHuPNwXsv6mkhgV2c4pXQUY8\n/7FiBvdcPoVPTh/l+9zJoxJ9+2QlxvDYzQuJcTu45uEP+M+Xd/lqNd5AcE5uMl+7cAIPf2EuC8al\nMTYtDrdTeM3u7J6ak8QlU7O5a/lkrjwnMMEDYH5BGgl+/QYAc/JTWDFrNHd9ckpAn4K/q+fl8cMV\n0zknL5nYKCez8lPI92sBGJ8Zz5i0uNPK5olyOXzBee6Y1ICbx0/Psn6XUI8f8Ap5IBCRBOBZ4F+N\nMb2vbJuBscaYWcCDwN+CHcMY84gxZr4xZn5mZmZoCzwAb0//qvetJhb/pqGBuJwO0hOiqWhsp6a5\n09euuHhCOlmJ0TzvdyH9yWt7SIiy/jh3lw+u/2BHWQNf/tNGni4K3ua//lANY9LiuGv5ZI7WtjI2\nPY4vLBpDVVN70Av2zmP1vL2vks1Bquy9Pfz2Ab711y2+DJa1eyuoae7wZYMYY3yLfA9GQ1snX/lz\nETuP1Q/6PWdaZ7eHHWX1I2KMx4tbj/GXDUd8E855/3ayk2KCBgL/fPO5Y1KJdjnYfKSO6aOTyEjo\nSXs8vzCd9Qer+4w1eHrTUQ5UNnPX8ikkx7m5ZGo2DW1djE6OISc5lsmjEgMyh3aVN9Dc0d0nEIDV\nMesdRNWf/LQ4/vKVRVwwMZNV75dw+f3vUFrTwvayBnJTYkmLjyLG7WT5jBxEBJfTwbiMeBrbushI\niCYzMZool4NbLyocdMetiDUf0XULg9cGBuPWiwp5+ZtLTvn9XpdMtaaN8DYLeX1iYiafnJ7tCwih\nFtJAICJurCDwuDHmud7bjTENxpgm+/ErgFtEMkJZptORlxrHlFGJlNW1khjjGjCHuLfspGhO2DUC\nb0eT0yGsnD2at/ZWUNvcwXv7q1i7t5LblhWSmxLrS33bcKiGxf+1hgp7EFFxRRPLfvaWb8I77xfz\njSD9Ch6Poci+Y/vi4gJuPK+A//rsOb4R1MGah/60rgSA8vo2mtoHngZgV3kDxuAbxu/913vcN3ZX\nsOD/rRl0M9fDbx1g9a4T/GXDkUHtH8x3ntnGvS/tOuX3//btA1z54HvcvKqo30kFh0NjWyffe347\n9722h4Y2a8K5wEDQ0zTU7TEcrm5mXEZP80iM2+lrt/dm6XidX5hBY3uXb+QqWKODf7l6H/PGpnLZ\nNOsC5W12mWNfqCZnJ7K/otHX/OftHwgWCAZrfGYCv75+LmvuvBCPgV+9sZ8dZfXMyE0Kuv9Eu59g\nak5i0O3DIcrl8A0QOx2XTR/FVXPzuGJmTp/j//aG+b7/v1ALZdaQAL8HdhtjftHPPqPs/RCRBXZ5\n+tZXRxBvBB+bHjekuT+yEmOsPoKWjoBMg5Wzc+nstkY3fv3xTYzLiOfG8wqYmpPEHrtp6O/bjlFe\n38Zbe6xmsVe2l3Owqtk3RN9bnX6vuKrP/C0HKpuobelkQUEaTofwwxXTOa8wg7xUKxCU9Vogp7qp\nnRe2HiMv1Wpn7m8kKVh3zvvsZoJNJTVUNbX7AkCpfQHddrSOjm4Pmw/3P0eM17G6Vt+ApbV7Kk/p\njryts5u/bSk75QU8jDE8t7mM3JRYPjxYzaW/eIf739hPa8fwN3U9sf4IjW1d1LV0sv6g9bXwtsNn\nJUVTXt/qO0dlta10dps+HYvnF1r3VRdMDAwEC+30Zf95sv7w3iEqGtv57qem+P62L5qcyaz8FK48\nx7pQTRqVSFunx1fj+/BgDQXpcb5MptMxNt3623/uo6McqmruN23Smx00bXTwQHE2SYh28fPPzeqT\nrj7cQlkjOB+4AVjmlx76KRH5moh8zd7namCHiGwFHgCuNSOhPj4Ab/PQ2LSh9eRbNQKrszg1vudO\nYvroJCZmJfD4+iMkx7p57F8WEuN2MjUnkYNVzbR1drPugHUReNfu3PPOZb7PTj09WNWE0yF0dHl4\nt9c85xvs/oFze92xeS/0ve94n9xYSkeXh/97xVTACiT92X+iiQ67aaHocC1bjvRc7L0XCm9K47ay\nkweCX6zehzFw+7JCyupafdkp247W0XySmonX5iO1tHd5OFrbekqBZEdZAwermrl9WSGvf+sClk3J\n4pdv7OOSX7w9rDN+tnV28+h7h5icbd31eqcoSbP/ds4tSONEQzvr7Hb+g1XW/9O4zMC/y+sWjuE/\nV073pUR6ZSZGMyEz3jeVQn1LJw+/fZDLpmUzb2zPvtEuJy/cdj6X24HAW549xxvxeAwbg/QPnI6v\nXzjB1zTaX/u4d9S/N4tJnb5QZg29Z4wRY8xMv/TQV4wxDxtjHrb3ecgYM90YM8sYs8gY836oynOm\nzMpLYUZuEovGD+2PPzMxhmp7ebs0vxqBiPDVCycwLSeJv3xlkS9FbWpOEt0ew/sHqiiuaCLK6WBd\ncRUNbZ2+UZ7e0ZQHK5s5vzCDxBhXn0mrNh6qISMh2jeHkldWYgwuhwQ0DXV7DI9/eJjzC9O5eGo2\nLodwoCJwDQZ/3tkTlxRmsKW0jqLDtTjtYfLe43oH4G0/2n+bvzGGP39QwrObj3Lj+QV8YZGVJfLm\nngo2Ha5hxUPrAqb7rrVHaQfzvn1hbO3sPqWFf/62pYwop4PLZ+SQlxrHr6+fy5O3LKKysZ37XtvT\n7/t6f1ZpzdBXtPP3/EdlVDa28+9XTiM3JZbVdrOfd/77f5qbS3ZStG/Cw232+e09J01yrJsbFhf4\nOpD9LRyfTlFJLV3dHv6x6zhN7V3ctrRwwHJ5s3a2l9Wxr6KR+tZOFti1izMhNT6KW5cWEu1yMCsv\nJeg+F0zM5Op5eVw4KXz9hR83OrJ4iBwO4eXbP8ENiwuG9L7spGiMAWMISLMDK0PhlTs+EZCVMMXO\nrvjDeyUAXL9oDDXNHfzxvRK6PIac5Bj22ndlJdXNTMpK4KLJWby5J3AK3Q2Halg4Lq1PM5bTIYxO\niQ3IHFpXXMWx+jY+v2AMbqeDMelx/eaag9WpHOt2cvW8PFo6unl281GmjEqkMCuB0toWjDE9gaCs\nPmg6arfHcPez2/n+CztZNjmLOy6eSHZSDDNyk3hj1wl++OIuXA7hjd0neHXHcd4vrmLxf6/hx6/s\nDlqmdQeqfJkcpQNkRQXT7TG8tPUYF03ODOh4XDQ+nZs/MY7nNpfx0ZG+HeglVc3M/9Fqntpo9Wts\nO1rHhT9dy9ObTm3AXnN7Fw+u2c85ucmcX5jOwnFpNLZZNSLvTUS0y8ktF0zgw4M1/HL1Ph5Ys5/z\nC9NJjx/8AKeF49JosgdmvbK9nLzUWN9o3P7ERbm4cFImv3v3kG9Jxt61jdP1tQvH88E9F/f5nngl\nx7n52TWz+iwKo06dBoJh4s39hsGNRhybHk+M28F7xVUkx7q5xc6FfuSdA1a63bw8yupaOVDZRFun\nh/GZCVwyNYvq5g7fxWr70XqO1bexeELwO7a81NiApqGnikpJiXNzqd1ROCEzYcCmoZ3HGpiSk+hr\ndqpsbGd2fgr5abGU1rRS2WRNeTA1J4nGti7fqGx/b++r4KmiUr564Xh+98X5vnS+ZZOz2Hykju1l\n9fzk6plMH53E//3bDm5atZG2Tg/Pf1Tmm0enuKKREw1tNLR1su1oPUvtAUZD7ej98GA1FY3tfGZO\n3+Euty0tJCsxmh++tKtPQNtSWofHwH2v7aW+pZP/fHkXHkPQgVKD8cCa/Ryrb+MHn56GiPjmpYLA\nm4jPL8gnPT6K+9fsZ2J2Ir/5wrwh9VstGm/9XazedYJ1xVVccU7OoN7/i8/NIjMhmr8WHSUnOcbX\nzHim+E/hoIaHBoJh4h0NCn1rBME4HcLkUVYb6OLx6eQkxzIpO4Hmjm4WjU9nut1++rrdFDQuI56l\nU7JIjHbxu3cPAvDHdYeIj3L2mUDPy78Jp7a5g9U7T/CZ2bm+kZYTMhMoqW4O2sRhjGH3sQamj05i\ndHKMbxGfOWNSyU+No76109cctMJOgQs2j/xrO46TGO3izksnBzRfLLM75c8tSOWzc3K576qZ1Ld2\nMiYtjp9cPZO6lk7e2VdJbXMHn/31+3z6wfd4uugo3R7DP59rTSDm/d0GWn7Q3xu7TxDtcrBsSlaf\nbQnRLu6+fApbS+v46et7A7btLm/A5RBqWzq44Q/r2VhSS1yU05cLPxR7jzfy+/cO8bn5ecy3M0a8\nTS9Oh5AU05P3Hhfl4jvLpzB3TAqrbjp3yFks2UkxFKTH8ei7h+jsNr5+gJNJT4jmkS/OI8btYPGE\n9JAumKKGhwaCYeJfI0gbZJV2qt08dF6hdSFYUphp/5vBpF6diBMyrWkFbloyjn/sPMHavRW8tO0Y\n18zP7/cCkZcaR0VjO+1d3bywpYyObg+fm98zC+OEzHg6u01AE4sx1pTApTWtNLZ3MS3Hmi5gXoGV\nXjjbb+CNt+P6sunZRLkc7Cir51hdKz96eRd1LR10dXt4Y3cFy+z5YvzNzE3m/1w2iZ9ePQsRYUZu\nMn//5hKe+fp5fHZOLqlxbl7YeoyH3zlAU0cX7V0e/vPlXUS7HHxiYgYpcW5fjeC7z2/nC48GG8sY\naGNJDXPGpPjmk+nts3NyuW7hGH7z1gH+94MS3+u7jzcyeVQi156bz7aj9UwZlciN5xWwv6LppAPr\nHn77AK/tKPed239/YQcJMS7uvnyqb5+C9DiyEqN9AxH9fe7cfJ679fyAv6+hWDgundbObnJTYpk1\nhDltpo9O5tU7LuAHVwYdA6rOMsGH1akzLiMhChFvH8Hg7tzOyUvmyY2lLLFTAK+YOYqnNh7h0mnZ\n5KXGEe1ysL2snvgoJ5mJVo3j5k+M40/vl/D1xzbR2W18w/OD8VbpS2taeXz9EWbkJgWk5HmH7BdX\nNDEuI55jda18+69bOFLdwoV288t0e/+r5+bR3ulhfEa8L4X13f2VuJ3W6m7TcpLYWGIt67m9rJ4u\nj2H5jFHUNHf4Rp76cziEbyybGPDalFE9ZbtiZo5v1beVs0Zz05JxXPe79ZxbkEqM20leqtU8ZYxh\n7d4KezBfR0CTQ1FJDRtKarj1okIa2zrZdayBbwzQWSoi3LtiOhUNbfzgxZ3ML0hjak4Su8utGTXv\nvGwy5fVt3L5sIpWN7XR7DHuONzI7P3inZ31LJz/9x17i3E7mjkll85FaNhyq4UefmRFQThHhsunZ\nA/bXnKqF49N4qqiUy2eMGvKdfagXS1HDRwPBMHE5HaTHR1PV1D7oGQuvmZfPtJwkxttzrM8bm8bO\ne5f7tk/ITGBXeQPjM3uW1UyKsfoTfvqPvSydnOl7bzC5diC4f81+9lc08dB1cwK2e997oLKJ7h0e\nvvPsdjq7PeSnxvGXDUfs5iurZrJ0ShZL7SaVfHuMwoHKZsZnxuNyOpiZl8yfP7BGZM/OT+GxDw9T\nWtNClMtxStkfK2fn8tiHVhn+9ZJJFGTEs+bOC4my52XJT41j34lGyuvbfAOvPjhQHTBw5/41+3l3\nfxWXz8jhSE0LHtM3zbY3l9PBT66exbwfrea1HcfJTIymsrGdqTmJZCRE86cvLwB6+ie2l9UzNSeR\nq37zPl8+bxxX+c10+dY+q2O/qaOLH7y4k13lDUzKTuDac/P7fO5/rJgx5HM0GBdNzmLx+HTfPEIq\nMmkgGEZZidE0tHUSFxW86aG3KJeDOWNS+90+eVSiHQgC78xuPK+A7UfruXXpwMP7vTWCl7Ye8y0W\n4i851k1mYjSPvHOQmuYOZuYl88C1c8hPi+PJjUdo7egO2oySEucmIdpFU3uXb4CTlQp4mNuXFXLD\n4rFc9NO3WLOngkumZvU738tA5o1JZcqoRBZPSKfA/gz/QU15qbF2+mlPh+26A1W+QFDf2skH9viM\nv31URrfH4HRIwEyV/UmLj2JOfgpv7a3wjfzsndOemxJLapybnWX1vBbjYkdZA396vyQgEKzedYKM\nhGiuW5DPA/YMlKtuWhB0krFQrVCVFh/FX25ZFJJjq7OHBoJhlJ0UTU1zxxnrXPPmdPeuosdHu3j4\nhnknff+opBicDqHbY/jeFdOClmtydiLrDlTx9Ysm8K1LJvna8q9f2H+Tk4iQl2pNkVFgT8x3xcwc\n4qNdXDotG6dD+NqFE/jF6n1cFqRZaDAcDuHVOz7R7/b8tDjauzys3mV1AC8anx4w2+Zbeyvo8hiy\nEqN5YUsZWUkxTB+dNOigtHRyFj9fvY93i60RzFN6BQJvv8b2snpKqntSaA9UNjEh01ph7O19lXxq\nRg63Li3kHztPMC4jXnPjVVhoZ/EwunpePl88r/8L6FBNsudcGaj5ZyAup4PJ2Yl8ZvboPpNeef34\ns+fw0jeW8J3lU/p06A7E22HsvVu3Jg4b5buzveWC8fz4s+ewsp+MpsEQkX6Dqre288buE5yTm8xF\nkzM5XN3iG/H8un03/u1LJ1FS3cLGkpohzevibQZ74sMjZCdFB013nJGbzO7yBj48WMON5xUgYs3v\nD1bHdGNbFxdPzSLG7eSl25fwP9fPHdLvr9SZooFgGF0xM4dbLxp45OZQLJmYwa0XTWDp5FO/i3zu\n1vP42TWz+t0+Jj3ulKbC9V6I++tQjHE7uW7hmD6Lgpwp3rmUWjq6mTs21dfhvq64ylpOdG8ll07L\n4lMzc4hyOTCGIQWCaTlJZCZG09jexdR+pjqYMToZjwGXQ7h16QTOm5DOC1vKMMb4UlWXTLTKFeVy\nBB39q9Rw0EBwFotxO7lr+RQST2MWxBi3MyQLX1gd2IGLiQ8n/0FOc/JTKMxKICsxmr9vL+epjaU0\ntXdx6bRskmLcXGLPH3Vuwcn7B7wcDvEFYP9sJn/e2TMvm55NVmIMK2fncri6hbuf3c4zm45yfmEG\ncVHaOqvCTwOBColr5uXxzNcWMyr59GelPBVxUS7fdAtzx1qLflw8NZt391fx/Rd2Eh/l9K3U9Z3l\nU/jF52aR7jdf/2B4RzD3Nx3ymLQ47rx0EndeNhmA5TNGEet28vSmUuaNTeWey6ec6q+n1BklI3yy\nzz7mz59vioqKwl0MdRZY+et1VDW2s+7uZYC1mtb+iiaO1LSQmRg9qAyhgXR2e3hyYynXzMvrdxBa\nbwcqm0iIdp2RaZuVGgoR2WSMmR9sm9ZL1cfWbRdNoK2rZ3oMl9PB1Jykftv0h8rtdHDDoqF1/oer\nqUypgWggUB9bp5qaqlSk0T4CpZSKcBoIlFIqwmkgUEqpCKeBQCmlIpwGAqWUinAaCJRSKsJpIFBK\nqQingUAppSLcWTfFhIhUAoeH+LYMoOqke4WXlvHM0DKeGVrG0zfSyjfWGBN0quKzLhCcChEp6m+O\njZFCy3hmaBnPDC3j6Rvp5fOnTUNKKRXhNBAopVSEi5RA8Ei4CzAIWsYzQ8t4ZmgZT99IL59PRPQR\nKKWU6l+k1AiUUkr1QwOBUkpFuI99IBCR5SKyV0SKReTucJcHQETyRWStiOwSkZ0icof9epqIrBaR\n/fa/p7eW4umX0ykiH4nIy/bzcSKy3j6XT4lIVJjLlyIiz4jIHhHZLSKLR+A5/Jb9f7xDRP4iIjHh\nPo8i8gcRqRCRHX6vBT1vYnnALus2EZkbxjL+1P6/3iYiz4tIit+2e+wy7hWRT4arjH7b7hQRIyIZ\n9vOwnMfB+lgHAhFxAr8GLgemAZ8XkWnhLRUAXcCdxphpwCLgNrtcdwNrjDETgTX283C6A9jt9/w+\n4JfGmEKgFrg5LKXqcT/wmjFmCjALq6wj5hyKSC7wTWC+MWYG4ASuJfzn8U/A8l6v9XfeLgcm2j+3\nAL8JYxlXAzOMMTOBfcA9APZ351pguv2e/7G/++EoIyKSD1wGHPF7OVzncVA+1oEAWAAUG2MOGmM6\ngCeBlWEuE8aYcmPMZvtxI9YFLBerbKvs3VYBnwlPCUFE8oArgEft5wIsA56xdwl3+ZKBC4DfAxhj\nOowxdYygc2hzAbEi4gLigHLCfB6NMe8ANb1e7u+8rQT+bCwfAikikhOOMhpjXjfGdNlPPwTy/Mr4\npDGm3RhzCCjG+u4PexltvwTuAvwzccJyHgfr4x4IcoFSv+dH7ddGDBEpAOYA64FsY0y5vek4kB2m\nYgH8CuuP2bv6ezpQ5/dFDPe5HAdUAn+0m68eFZF4RtA5NMaUAT/DujMsB+qBTYys8+jV33kbqd+h\nm4BX7ccjpowishIoM8Zs7bVpxJQxmI97IBjRRCQBeBb4V2NMg/82Y+X1hiW3V0SuBCqMMZvC8fmD\n5ALmAr8xxswBmunVDBTOcwhgt7OvxApao4F4gjQljDThPm8nIyLfw2pefTzcZfEnInHAd4Hvh7ss\nQ/VxDwRlQL7f8zz7tbATETdWEHjcGPOc/fIJb3XR/rciTMU7H1ghIiVYzWnLsNrjU+wmDgj/uTwK\nHDXGrLefP4MVGEbKOQS4BDhkjKk0xnQCz2Gd25F0Hr36O28j6jskIjcCVwLXm55BUCOljBOwgv5W\n+7uTB2wWkVGMnDIG9XEPBBuBiXaWRhRWh9KLYS6Tt73998BuY8wv/Da9CHzJfvwl4IXhLhuAMeYe\nY0yeMaYA65y9aYy5HlgLXB3u8gEYY44DpSIy2X7pYmAXI+Qc2o4Ai0Qkzv4/95ZxxJxHP/2dtxeB\nL9pZL4uAer8mpGElIsuxmitXGGNa/Da9CFwrItEiMg6rQ3bDcJfPGLPdGJNljCmwvztHgbn23+qI\nOY9BGWM+1j/Ap7AyDA4A3wt3eewyLcGqem8Dttg/n8Jqh18D7AfeANJGQFkvAl62H4/H+oIVA08D\n0WEu22ygyD6PfwNSR9o5BP4D2APsAP4XiA73eQT+gtVn0Yl1sbq5v/MGCFbm3QFgO1YGVLjKWIzV\nzu79zjzst//37DLuBS4PVxl7bS8BMsJ5Hgf7o1NMKKVUhPu4Nw0ppZQ6CQ0ESikV4TQQKKVUhNNA\noJRSEU4DgVJKRTgNBErZRKRbRLb4/ZyxCetEpCDYLJVKjQSuk++iVMRoNcbMDnchlBpuWiNQ6iRE\npEREfiIi20Vkg4gU2q8XiMib9vzya0RkjP16tj1f/lb75zz7UE4R+Z1Y6xO8LiKx9v7fFGttim0i\n8mSYfk0VwTQQKNUjtlfT0D/7bas3xpwDPIQ1MyvAg8AqY82P/zjwgP36A8DbxphZWPMf7bRfnwj8\n2hgzHagDrrJfvxuYYx/na6H65ZTqj44sVsomIk3GmIQgr5cAy4wxB+3JAo8bY9JFpArIMcZ02q+X\nG2MyRKQSyDPGtPsdowBYbayFXxCR7wBuY8yPROQ1oAlrmoy/GWOaQvyrKhVAawRKDY7p5/FQtPs9\n7qanj+4KrHlo5gIb/WYmVWpYaCBQanD+2e/fD+zH72PNzgpwPfCu/XgN8HXwrfuc3N9BRcQB5Btj\n1gLfAZKBPrUSpUJJ7zyU6hErIlv8nr9mjPGmkKaKyDasu/rP26/djrVC2r9hrZb2Zfv1O4BHRORm\nrDv/r2PNUhmME3jMDhYCPGCsJTeVGjbaR6DUSdh9BPONMVXhLotSoaBNQ0opFeG0RqCUUhFOawRK\nKRXhNBAopVSE00CglFIRTgOBUkpFOA0ESikV4f4/WPUIJeDZWpAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSa31fMeAY1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nn_j13zAqpC",
        "colab_type": "text"
      },
      "source": [
        "It may be a bit hard to see the plot due to scaling issues and relatively high variance. Let's:\n",
        "\n",
        "Omit the first 10 data points, which are on a different scale from the rest of the curve.\n",
        "\n",
        "\n",
        "Replace each point with an exponential moving average of the previous points, to obtain a smooth curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br0cYSiBA2tF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpC8aFJCA28b",
        "colab_type": "code",
        "outputId": "9afd2ed7-c6f6-4cc1-bc2f-64e413e6c6a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "def smooth_curve(points, factor=0.9):\n",
        "  smoothed_points = []\n",
        "  for point in points:\n",
        "    if smoothed_points:\n",
        "      previous = smoothed_points[-1]\n",
        "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
        "    else:\n",
        "      smoothed_points.append(point)\n",
        "  return smoothed_points\n",
        "\n",
        "smooth_mae_history = smooth_curve(average_mae_history[10:])\n",
        "\n",
        "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation MAE')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH3Jw46FA3NJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf3aB6MW_sNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8E81NUUBAEs",
        "colab_type": "text"
      },
      "source": [
        "According to this plot, it seems that validation MAE stops improving significantly after 80 epochs. Past that point, we start overfitting.\n",
        "\n",
        "\n",
        "Once we are done tuning other parameters of our model (besides the number of epochs, we could also adjust the size of the hidden layers), we can train a final \"production\" model on all of the training data, with the best parameters, then look at its performance on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpQ77eikA9Xx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVsytgg3A9f4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get a fresh, compiled model.\n",
        "model = build_model()\n",
        "# Train it on the entirety of the data.\n",
        "model.fit(train_data, train_targets,\n",
        "          epochs=80, batch_size=16, verbose=0)\n",
        "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23QckeH-A9c-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHRgq5PTA9Sy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Here's what you should take away from this example:\n",
        "\n",
        "Regression is done using different loss functions from classification; Mean Squared Error (MSE) is a commonly used loss function for regression.\n",
        "Similarly, evaluation metrics to be used for regression differ from those used for classification; naturally the concept of \"accuracy\" does not apply for regression. A common regression metric is Mean Absolute Error (MAE).\n",
        "When features in the input data have values in different ranges, each feature should be scaled independently as a preprocessing step.\n",
        "When there is little data available, using K-Fold validation is a great way to reliably evaluate a model.\n",
        "When little training data is available, it is preferable to use a small network with very few hidden layers (typically only one or two), in order to avoid severe overfitting."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfF6PrpaGKxW",
        "colab_type": "text"
      },
      "source": [
        "# Here's what you should take away from this example:\n",
        "\n",
        "Regression is done using different loss functions from classification; Mean Squared Error (MSE) is a commonly used loss function for regression.\n",
        "\n",
        "\n",
        "Similarly, evaluation metrics to be used for regression differ from those used for classification; naturally the concept of \"accuracy\" does not apply for regression. A common regression metric is Mean Absolute Error (MAE).\n",
        "\n",
        "\n",
        "When features in the input data have values in different ranges, each feature should be scaled independently as a preprocessing step.\n",
        "\n",
        "\n",
        "When there is little data available, using K-Fold validation is a great way to reliably evaluate a model.\n",
        "When little training data is available, it is preferable to use a small network with very few hidden layers (typically only one or two), in order to avoid severe overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHt50Xw-GP_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}